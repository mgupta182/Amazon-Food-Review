{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Amzon Fine Food with Imbalanced .ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqWtssCF_Hlr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8365edb-29da-4348-e912-3cd4ab2cc847"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EA-uWnT8AQSz"
      },
      "source": [
        "#Importing required libraries\n",
        "\n",
        "#for reading and plotting\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#for performance evaluation\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "#for feature engineering\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "#models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "#for hyperparameter tuning\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import string\n",
        "import re\n",
        "import pickle\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTjts3WsBfeQ"
      },
      "source": [
        "#loading data \n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/DataSets/Amazon Reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0pd_NiVBtRH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "a21e53e5-a4a6-40ff-89a6-e778a8ecd545"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>ProductId</th>\n",
              "      <th>UserId</th>\n",
              "      <th>ProfileName</th>\n",
              "      <th>HelpfulnessNumerator</th>\n",
              "      <th>HelpfulnessDenominator</th>\n",
              "      <th>Score</th>\n",
              "      <th>Time</th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>B001E4KFG0</td>\n",
              "      <td>A3SGXH7AUHU8GW</td>\n",
              "      <td>delmartian</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1303862400</td>\n",
              "      <td>Good Quality Dog Food</td>\n",
              "      <td>I have bought several of the Vitality canned d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>B00813GRG4</td>\n",
              "      <td>A1D87F6ZCVE5NK</td>\n",
              "      <td>dll pa</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1346976000</td>\n",
              "      <td>Not as Advertised</td>\n",
              "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>B000LQOCH0</td>\n",
              "      <td>ABXLMWJIXXAIN</td>\n",
              "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1219017600</td>\n",
              "      <td>\"Delight\" says it all</td>\n",
              "      <td>This is a confection that has been around a fe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>B000UA0QIQ</td>\n",
              "      <td>A395BORC6FGVXV</td>\n",
              "      <td>Karl</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1307923200</td>\n",
              "      <td>Cough Medicine</td>\n",
              "      <td>If you are looking for the secret ingredient i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>B006K2ZZ7K</td>\n",
              "      <td>A1UQRSCLF8GW1T</td>\n",
              "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1350777600</td>\n",
              "      <td>Great taffy</td>\n",
              "      <td>Great taffy at a great price.  There was a wid...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Id  ...                                               Text\n",
              "0   1  ...  I have bought several of the Vitality canned d...\n",
              "1   2  ...  Product arrived labeled as Jumbo Salted Peanut...\n",
              "2   3  ...  This is a confection that has been around a fe...\n",
              "3   4  ...  If you are looking for the secret ingredient i...\n",
              "4   5  ...  Great taffy at a great price.  There was a wid...\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5A2nFpYUBxqf",
        "outputId": "70bbc75d-ab9f-4f5d-927e-353f367d0482"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(568454, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcwowOf8B1UU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b08e8ac9-1e66-4790-ed61-049f8dbe6421"
      },
      "source": [
        "#Distribution of class present in the data\n",
        "\n",
        "df['Score'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5    363122\n",
              "4     80655\n",
              "1     52268\n",
              "3     42640\n",
              "2     29769\n",
              "Name: Score, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqryhijHB4jB",
        "outputId": "f164633f-8f03-4898-b443-c363e0b3b0cb"
      },
      "source": [
        "#checking for null values\n",
        "\n",
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Id                         0\n",
              "ProductId                  0\n",
              "UserId                     0\n",
              "ProfileName               16\n",
              "HelpfulnessNumerator       0\n",
              "HelpfulnessDenominator     0\n",
              "Score                      0\n",
              "Time                       0\n",
              "Summary                   27\n",
              "Text                       0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXptqN3vB7MJ"
      },
      "source": [
        "#df[pd.isnull(df).any(axis=1)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUaPe-dZB_kB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "outputId": "056a4e1b-3d81-4436-adc8-a265ed9088d3"
      },
      "source": [
        "#looking for duplicate values and treating them\n",
        "\n",
        "df[df.duplicated(['UserId','ProfileName','Score','Time'])]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>ProductId</th>\n",
              "      <th>UserId</th>\n",
              "      <th>ProfileName</th>\n",
              "      <th>HelpfulnessNumerator</th>\n",
              "      <th>HelpfulnessDenominator</th>\n",
              "      <th>Score</th>\n",
              "      <th>Time</th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>30</td>\n",
              "      <td>B0001PB9FY</td>\n",
              "      <td>A3HDKO7OW0QNK4</td>\n",
              "      <td>Canadian Fan</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1107820800</td>\n",
              "      <td>The Best Hot Sauce in the World</td>\n",
              "      <td>I don't know if it's the cactus or the tequila...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>426</th>\n",
              "      <td>427</td>\n",
              "      <td>B000G6RYNE</td>\n",
              "      <td>A1Y3XPZK9ZADFW</td>\n",
              "      <td>albinocrow</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1334016000</td>\n",
              "      <td>pretty good, could be better</td>\n",
              "      <td>Glad to find these in a one ounce size but the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>467</th>\n",
              "      <td>468</td>\n",
              "      <td>B000G6RYNE</td>\n",
              "      <td>A3PJZ8TU8FDQ1K</td>\n",
              "      <td>Jared Castle</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1231718400</td>\n",
              "      <td>Crunchy, salty, sweet...finally, a Superbowl s...</td>\n",
              "      <td>These Honey Dijon chips bring a terrific balan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>486</th>\n",
              "      <td>487</td>\n",
              "      <td>B000G6RYNE</td>\n",
              "      <td>A31N6KB160O508</td>\n",
              "      <td>Fran W.</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1214006400</td>\n",
              "      <td>Firm quality chip</td>\n",
              "      <td>Just got this order and it is ok. A bit light ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>574</th>\n",
              "      <td>575</td>\n",
              "      <td>B000G6RYNE</td>\n",
              "      <td>A3PJZ8TU8FDQ1K</td>\n",
              "      <td>Jared Castle</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>1231718400</td>\n",
              "      <td>One bite and you'll become a \"chippoisseur\"</td>\n",
              "      <td>I'm addicted to salty and tangy flavors, so wh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568411</th>\n",
              "      <td>568412</td>\n",
              "      <td>B0018CLWM4</td>\n",
              "      <td>AUX1HSY8FX55S</td>\n",
              "      <td>DAW</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1319500800</td>\n",
              "      <td>Happy Camper</td>\n",
              "      <td>I bought this to try on two registered Maine C...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568412</th>\n",
              "      <td>568413</td>\n",
              "      <td>B0018CLWM4</td>\n",
              "      <td>AVZ2OZ479Q9E8</td>\n",
              "      <td>Ai Ling Chow</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1336435200</td>\n",
              "      <td>Two Siberians like it!</td>\n",
              "      <td>When we brought home two 3-month-old purebred ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568413</th>\n",
              "      <td>568414</td>\n",
              "      <td>B0018CLWM4</td>\n",
              "      <td>AI3Y26HLPYW4L</td>\n",
              "      <td>kimosabe</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1330041600</td>\n",
              "      <td>premium edge cat food</td>\n",
              "      <td>My cats don't like it. what else can I say to ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568440</th>\n",
              "      <td>568441</td>\n",
              "      <td>B005ZC0RRO</td>\n",
              "      <td>A2TO5R8QLIITEF</td>\n",
              "      <td>SAK</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1323734400</td>\n",
              "      <td>Delicious, all natural and allergy free treats!</td>\n",
              "      <td>Indie Candy's gummies are absolutely delicious...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568445</th>\n",
              "      <td>568446</td>\n",
              "      <td>B001EO7N10</td>\n",
              "      <td>A2E5C8TTAED4CQ</td>\n",
              "      <td>S. Linkletter</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>1268006400</td>\n",
              "      <td>Five Spice Powder</td>\n",
              "      <td>You can make this mix yourself, but the Star A...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>200361 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            Id  ...                                               Text\n",
              "29          30  ...  I don't know if it's the cactus or the tequila...\n",
              "426        427  ...  Glad to find these in a one ounce size but the...\n",
              "467        468  ...  These Honey Dijon chips bring a terrific balan...\n",
              "486        487  ...  Just got this order and it is ok. A bit light ...\n",
              "574        575  ...  I'm addicted to salty and tangy flavors, so wh...\n",
              "...        ...  ...                                                ...\n",
              "568411  568412  ...  I bought this to try on two registered Maine C...\n",
              "568412  568413  ...  When we brought home two 3-month-old purebred ...\n",
              "568413  568414  ...  My cats don't like it. what else can I say to ...\n",
              "568440  568441  ...  Indie Candy's gummies are absolutely delicious...\n",
              "568445  568446  ...  You can make this mix yourself, but the Star A...\n",
              "\n",
              "[200361 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-_9-Wfoz3Ec"
      },
      "source": [
        "So there are some duplicates\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYf2onYyDpWV"
      },
      "source": [
        "#dropping duplicates\n",
        "\n",
        "df.drop_duplicates(inplace=True, subset=['UserId','ProfileName','Score','Time'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqUGlbFQDxxT",
        "outputId": "31b6539e-40cc-4c95-a2dc-81f35ab96e19"
      },
      "source": [
        "#shape of data after dropping duplicated values\n",
        "\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(368093, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxRAUelKD-OZ",
        "outputId": "34c9cec9-04a6-457f-9756-c65425d529d7"
      },
      "source": [
        "#again checking for null values\n",
        "\n",
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Id                         0\n",
              "ProductId                  0\n",
              "UserId                     0\n",
              "ProfileName               11\n",
              "HelpfulnessNumerator       0\n",
              "HelpfulnessDenominator     0\n",
              "Score                      0\n",
              "Time                       0\n",
              "Summary                    3\n",
              "Text                       0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xDWyuaV0ELt"
      },
      "source": [
        "Null values data is also dropped after duplicated values dropping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARevXl_OECt5",
        "outputId": "97fff460-84b8-4dc3-99c2-63351d1051e1"
      },
      "source": [
        "#dropping null values\n",
        "\n",
        "total_rows =  df.shape[0]\n",
        "df.dropna(how='any',inplace=True)\n",
        "remaining_rows= df.shape[0]\n",
        "\n",
        "removed_rows = total_rows-remaining_rows\n",
        "print(\"No. of rows removed :\", removed_rows)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No. of rows removed : 14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nh8z80Q4EH32"
      },
      "source": [
        "#Removing Noise \n",
        "\n",
        "df = df[df['HelpfulnessNumerator']<=df['HelpfulnessDenominator']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7NGqTg2ENwq",
        "outputId": "d92c6d7e-e321-4f06-9779-0123c1c08319"
      },
      "source": [
        "#final dataframe shape after de-duplication, null-values treatment and removing noise\n",
        "\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(368077, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b86ctHyrEUyp"
      },
      "source": [
        "#renaming sentiment score with positive and negative\n",
        "\n",
        "def renaming_score(x):\n",
        "    \n",
        "    return \"Positive\" if x>=3 else \"Negative\"\n",
        "\n",
        "df.loc[:, 'sentiment'] = df.Score.apply(renaming_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 78
        },
        "id": "a9a2olbsEYVQ",
        "outputId": "689fee04-856a-40f1-fdc9-35c16738c35e"
      },
      "source": [
        "df.head(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>ProductId</th>\n",
              "      <th>UserId</th>\n",
              "      <th>ProfileName</th>\n",
              "      <th>HelpfulnessNumerator</th>\n",
              "      <th>HelpfulnessDenominator</th>\n",
              "      <th>Score</th>\n",
              "      <th>Time</th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>B001E4KFG0</td>\n",
              "      <td>A3SGXH7AUHU8GW</td>\n",
              "      <td>delmartian</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1303862400</td>\n",
              "      <td>Good Quality Dog Food</td>\n",
              "      <td>I have bought several of the Vitality canned d...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Id   ProductId  ...                                               Text sentiment\n",
              "0   1  B001E4KFG0  ...  I have bought several of the Vitality canned d...  Positive\n",
              "\n",
              "[1 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6Nly4oZEdfr",
        "outputId": "e9e404f0-4113-434b-ed60-ebb56088e63b"
      },
      "source": [
        "#looking for class distribution\n",
        "\n",
        "df['sentiment'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Positive    312340\n",
              "Negative     55737\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "gP7eV_e0EiOr",
        "outputId": "cf1308cb-56d9-41f9-c9d1-355ed6bc2ebd"
      },
      "source": [
        "sns.countplot(data=df,x='sentiment')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f80f1589890>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsafvZj8Emt7"
      },
      "source": [
        "#df1 will be used with class imbalance\n",
        "\n",
        "df1 = df.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THjl_OSuEqkc",
        "outputId": "bf15235e-2e7b-4fb4-f3f1-270c4da565d4"
      },
      "source": [
        "#downsampling the classes taking 40000 random samples of each\n",
        "\n",
        "positive = df.loc[df.sentiment==\"Positive\"].sample(40000)\n",
        "\n",
        "negative = df.loc[df.sentiment==\"Negative\"].sample(40000)\n",
        "\n",
        "df = pd.concat([positive, negative])\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(80000, 11)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R09MKzfiE4yh",
        "outputId": "2c28fbae-b3db-4eb2-f5e9-6ee6c4d73793"
      },
      "source": [
        "#modifying stopwords vocabolary\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "total_stopwords = set(stopwords.words('english'))\n",
        "\n",
        "# subtract negative stop words like no, not, don't etc.. from total_stopwords\n",
        "negative_stop_words = set(word for word in total_stopwords \n",
        "                          if \"n't\" in word or 'no' in word or \"n'\" in word)\n",
        "\n",
        "final_stopwords = total_stopwords - negative_stop_words\n",
        "\n",
        "\n",
        "#final_stopwords.add(\"one\")\n",
        "print(final_stopwords)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "{'she', 'll', 'being', 'than', 'too', 'few', 'what', 'himself', 'we', 'or', 'can', 'same', 'did', 'hadn', 'herself', 'the', 'each', 'further', 'i', 'who', 'yourself', 's', 'won', 'between', 'some', 'doing', 'didn', 'needn', 'below', 'shan', 'your', 'down', 'it', 'other', 'once', 'hasn', 'yourselves', \"you'll\", \"you've\", 'with', 'does', 'above', 'their', 've', 'to', 'has', 'isn', 'theirs', 'been', 'why', 'me', 'couldn', 'our', 'm', 'about', 'those', 'by', 'myself', 'were', 't', 'him', 'are', 'an', 'off', 'should', 'y', 'and', 'his', 'hers', 'such', 'then', 'mustn', 'after', 'most', 'he', 'mightn', \"that'll\", 'for', 'ain', 'again', 'only', 'into', 'shouldn', 'a', 'weren', 'whom', 'them', \"it's\", 'on', 'wouldn', 'was', 'in', \"you're\", 'itself', 'out', 'ourselves', 'these', 'have', 'how', 'be', 'you', \"should've\", 're', 'her', 'my', 'as', 'own', 'where', 'so', 'while', 'having', 'am', 'at', 'during', 'don', 'its', 'if', 'here', 'wasn', 'ours', 'just', 'd', 'do', 'when', 'there', 'against', 'until', 'will', 'but', 'any', 'more', 'themselves', 'very', 'ma', 'because', 'haven', 'aren', 'o', 'of', \"she's\", 'up', 'yours', 'they', 'both', 'had', 'from', 'under', 'through', 'that', 'all', 'doesn', 'is', 'before', \"you'd\", 'over', 'which', 'this'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZyvjZY7E9Ui"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "\n",
        "#from nltk.corpus import stopwords\n",
        "stop = final_stopwords\n",
        "\n",
        "#initialising the snowball stemmer\n",
        "sno = nltk.stem.SnowballStemmer('english')                      \n",
        "\n",
        "#function to clean the word of any html-tags\n",
        "def cleanhtml(sentence): \n",
        "    cleanr = re.compile('<.*?>')\n",
        "    cleantext = re.sub(cleanr, ' ', sentence)\n",
        "    return cleantext\n",
        "\n",
        "#function to clean the word of any punctuation or special characters\n",
        "def cleanpunc(sentence): \n",
        "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
        "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
        "    return  cleaned\n",
        "\n",
        "\n",
        "#function to text summarization\n",
        "def final_sentence(text):\n",
        "    \n",
        "    text = text.split()\n",
        "    text = [cleanhtml(x) for x in text]\n",
        "    text = [cleanpunc(x) for x in text]\n",
        "    \n",
        "    def test(word):\n",
        "        if word.isalpha() and len(word) > 2 and word.lower() not in stop:\n",
        "            s=(sno.stem(word.lower()))\n",
        "            return s\n",
        "        else:\n",
        "            pass\n",
        "    text = [test(x) for x in text if test(x)]\n",
        "    \n",
        "    return ' '.join(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhfMI-853pgq",
        "outputId": "58126b48-0950-4fe1-a46f-576bd1bd0b97"
      },
      "source": [
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "df['CleanedText'] = df['Text'].progress_apply(final_sentence)\n",
        "df1['CleanedText'] = df1['Text'].progress_apply(final_sentence)\n",
        "print(df.shape)\n",
        "print(df1.shape)\n",
        "#df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 80000/80000 [02:10<00:00, 611.43it/s]\n",
            "100%|██████████| 368077/368077 [09:41<00:00, 632.83it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(80000, 12)\n",
            "(368077, 12)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 95
        },
        "id": "IkVg54RF3kZJ",
        "outputId": "dcdd0468-758c-43e6-d9ba-d4e032a84471"
      },
      "source": [
        "df.head(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>ProductId</th>\n",
              "      <th>UserId</th>\n",
              "      <th>ProfileName</th>\n",
              "      <th>HelpfulnessNumerator</th>\n",
              "      <th>HelpfulnessDenominator</th>\n",
              "      <th>Score</th>\n",
              "      <th>Time</th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>CleanedText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>96732</th>\n",
              "      <td>96733</td>\n",
              "      <td>B001ELL4F4</td>\n",
              "      <td>AD1QYPQT28F0U</td>\n",
              "      <td>P. Contino</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>1195171200</td>\n",
              "      <td>best cup of regular</td>\n",
              "      <td>If you like a bolder coffee with rich flavor, ...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>like bolder coffe rich not bitter burnt full f...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Id  ...                                        CleanedText\n",
              "96732  96733  ...  like bolder coffe rich not bitter burnt full f...\n",
              "\n",
              "[1 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 95
        },
        "id": "butQMCTs1_Js",
        "outputId": "11b6981e-c3ec-4b4b-e29c-0cf6c073a58e"
      },
      "source": [
        "df1.head(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>ProductId</th>\n",
              "      <th>UserId</th>\n",
              "      <th>ProfileName</th>\n",
              "      <th>HelpfulnessNumerator</th>\n",
              "      <th>HelpfulnessDenominator</th>\n",
              "      <th>Score</th>\n",
              "      <th>Time</th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>CleanedText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>B001E4KFG0</td>\n",
              "      <td>A3SGXH7AUHU8GW</td>\n",
              "      <td>delmartian</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1303862400</td>\n",
              "      <td>Good Quality Dog Food</td>\n",
              "      <td>I have bought several of the Vitality canned d...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>bought sever vital can dog food product found ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Id   ProductId  ... sentiment                                        CleanedText\n",
              "0   1  B001E4KFG0  ...  Positive  bought sever vital can dog food product found ...\n",
              "\n",
              "[1 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYM_-k3p75qI"
      },
      "source": [
        "# #word cloud\n",
        "\n",
        "# from wordcloud import WordCloud, STOPWORDS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGLg49NF6GmM"
      },
      "source": [
        "# def generate_wcloud(text):\n",
        "    \n",
        "    \n",
        "#     wordcloud = WordCloud(background_color='white')\n",
        "#     wordcloud.generate(text)\n",
        "    \n",
        "#     plt.figure(figsize=(15,7))\n",
        "#     plt.axis('off')\n",
        "#     plt.imshow(wordcloud, interpolation='bilinear')\n",
        "#     return plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKJeSfUp62xo"
      },
      "source": [
        "####Positive Reviews"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iXk0O4F6_QZ"
      },
      "source": [
        "# pos = df.loc[df.sentiment==\"Positive\"].Text\n",
        "# text = \" \".join(review for review in pos.astype(str))\n",
        "\n",
        "# generate_wcloud(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPUX49QF7DA3"
      },
      "source": [
        "###Negative Reviews"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSj7HblO8WgV"
      },
      "source": [
        "# pos = df.loc[df.sentiment==\"Negative\"].Text\n",
        "# text = \" \".join(review for review in pos.astype(str))\n",
        "\n",
        "# generate_wcloud(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGNQE38k8gN4"
      },
      "source": [
        "###Neutral Reviews"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EazphTTf9CGd"
      },
      "source": [
        "# pos = df.loc[df.sentiment==\"Neutral\"].Text\n",
        "# text = \" \".join(review for review in pos.astype(str))\n",
        "\n",
        "# generate_wcloud(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeludExl9G4K"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X = df.CleanedText\n",
        "y = df.sentiment\n",
        "\n",
        "X1 = df1.CleanedText\n",
        "y1 = df1.sentiment\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1, stratify=y)\n",
        "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.20)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjbFb5KK-sxG",
        "outputId": "515fa998-f94b-4d72-9fa3-6a5f7657aabb"
      },
      "source": [
        "X_train.shape, X_test.shape\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((64000,), (16000,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7Y5OJTU3PLu",
        "outputId": "7f80b098-79ca-4e5f-bbba-f869206b0ce5"
      },
      "source": [
        "X1_train.shape, X1_test.shape\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((294461,), (73616,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "str2lzyL5uZV",
        "outputId": "6fddff31-64dd-4438-ef12-14f1dc902c4c"
      },
      "source": [
        "y1_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(73616,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2M5N-N4p_j1L"
      },
      "source": [
        "# Implementing Bag Of Words\n",
        "bow_vectorizer = CountVectorizer(max_features=10000)\n",
        "bow_vectorizer1 = CountVectorizer(max_features=10000)\n",
        "bow_vectorizer.fit(X_train)\n",
        "bow_vectorizer1.fit(X1_train)\n",
        "# transformation\n",
        "bow_X_train = bow_vectorizer.transform(X_train)\n",
        "bow_X_test = bow_vectorizer.transform(X_test)\n",
        "bow_X1_train = bow_vectorizer1.transform(X1_train)\n",
        "bow_X1_test = bow_vectorizer1.transform(X1_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8mIQB-pAeiO",
        "outputId": "7a277c63-b4e6-4199-cf6f-e2a15516148f"
      },
      "source": [
        "bow_vectorizer.vocabulary_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dog': 2557,\n",
              " 'love': 5084,\n",
              " 'not': 5887,\n",
              " 'stand': 8294,\n",
              " 'jerki': 4570,\n",
              " 'best': 746,\n",
              " 'ive': 4524,\n",
              " 'ever': 2960,\n",
              " 'great': 3772,\n",
              " 'also': 245,\n",
              " 'appl': 358,\n",
              " 'slice': 7957,\n",
              " 'time': 8932,\n",
              " 'wish': 9804,\n",
              " 'could': 1962,\n",
              " 'find': 3207,\n",
              " 'local': 5038,\n",
              " 'chain': 1389,\n",
              " 'seem': 7671,\n",
              " 'carri': 1284,\n",
              " 'sever': 7733,\n",
              " 'worth': 9847,\n",
              " 'send': 7693,\n",
              " 'get': 3573,\n",
              " 'dont': 2577,\n",
              " 'want': 9609,\n",
              " 'tag': 8661,\n",
              " 'amazon': 264,\n",
              " 'there': 8841,\n",
              " 'run': 7447,\n",
              " 'might': 5451,\n",
              " 'avail': 510,\n",
              " 'next': 5816,\n",
              " 'especi': 2922,\n",
              " 'coffe': 1701,\n",
              " 'provid': 6854,\n",
              " 'aromat': 402,\n",
              " 'scent': 7601,\n",
              " 'air': 164,\n",
              " 'brew': 1013,\n",
              " 'fresh': 3416,\n",
              " 'well': 9684,\n",
              " 'ground': 3814,\n",
              " 'size': 7912,\n",
              " 'nice': 5822,\n",
              " 'make': 5175,\n",
              " 'cup': 2102,\n",
              " 'flavor': 3255,\n",
              " 'bit': 813,\n",
              " 'lighter': 4970,\n",
              " 'side': 7853,\n",
              " 'may': 5312,\n",
              " 'favorit': 3127,\n",
              " 'your': 9937,\n",
              " 'realli': 7058,\n",
              " 'dark': 2174,\n",
              " 'wolfgang': 9818,\n",
              " 'puck': 6869,\n",
              " 'sign': 7861,\n",
              " 'recip': 7085,\n",
              " 'wooden': 9828,\n",
              " 'tongu': 8984,\n",
              " 'doctor': 2550,\n",
              " 'offic': 5997,\n",
              " 'thin': 8861,\n",
              " 'like': 4974,\n",
              " 'chunki': 1570,\n",
              " 'tap': 8701,\n",
              " 'even': 2955,\n",
              " 'take': 8672,\n",
              " 'tini': 8941,\n",
              " 'dumpl': 2692,\n",
              " 'back': 546,\n",
              " 'gotta': 3707,\n",
              " 'gift': 3586,\n",
              " 'coupl': 1975,\n",
              " 'drink': 2645,\n",
              " 'lot': 5074,\n",
              " 'select': 7682,\n",
              " 'probabl': 6781,\n",
              " 'would': 9853,\n",
              " 'buy': 1158,\n",
              " 'product': 6797,\n",
              " 'return': 7288,\n",
              " 'give': 3602,\n",
              " 'certif': 1379,\n",
              " 'smell': 8002,\n",
              " 'fish': 3226,\n",
              " 'strong': 8438,\n",
              " 'crumbl': 2073,\n",
              " 'eat': 2736,\n",
              " 'chicken': 1485,\n",
              " 'allerg': 220,\n",
              " 'seen': 7672,\n",
              " 'beef': 683,\n",
              " 'veggi': 9446,\n",
              " 'stick': 8355,\n",
              " 'littl': 5024,\n",
              " 'price': 6754,\n",
              " 'purchas': 6906,\n",
              " 'better': 752,\n",
              " 'need': 5765,\n",
              " 'actual': 74,\n",
              " 'possess': 6663,\n",
              " 'almost': 231,\n",
              " 'exact': 2981,\n",
              " 'vanilla': 9420,\n",
              " 'tile': 8928,\n",
              " 'look': 5056,\n",
              " 'healthi': 3995,\n",
              " 'oatmeal': 5960,\n",
              " 'that': 8827,\n",
              " 'natur': 5742,\n",
              " 'path': 6300,\n",
              " 'tasti': 8727,\n",
              " 'one': 6028,\n",
              " 'compar': 1766,\n",
              " 'surpris': 8583,\n",
              " 'open': 6042,\n",
              " 'box': 961,\n",
              " 'found': 3370,\n",
              " 'proper': 6830,\n",
              " 'spill': 8208,\n",
              " 'insid': 4419,\n",
              " 'obvious': 5976,\n",
              " 'first': 3224,\n",
              " 'last': 4842,\n",
              " 'although': 249,\n",
              " 'tast': 8722,\n",
              " 'pay': 6315,\n",
              " 'trash': 9065,\n",
              " 'poor': 6636,\n",
              " 'where': 9712,\n",
              " 'qualiti': 6942,\n",
              " 'control': 1889,\n",
              " 'big': 771,\n",
              " 'fan': 3090,\n",
              " 'coconut': 1694,\n",
              " 'prefer': 6719,\n",
              " 'vita': 9544,\n",
              " 'slight': 7962,\n",
              " 'inconsist': 4325,\n",
              " 'across': 68,\n",
              " 'differ': 2403,\n",
              " 'issu': 4511,\n",
              " 'compani': 1764,\n",
              " 'orang': 6055,\n",
              " 'juic': 4617,\n",
              " 'oxygen': 6175,\n",
              " 'add': 81,\n",
              " 'chemic': 1459,\n",
              " 'produc': 6796,\n",
              " 'absolut': 19,\n",
              " 'suppos': 8570,\n",
              " 'peopl': 6368,\n",
              " 'imposs': 4296,\n",
              " 'everi': 2961,\n",
              " 'batch': 640,\n",
              " 'unless': 9296,\n",
              " 'modifi': 5549,\n",
              " 'coco': 1692,\n",
              " 'closest': 1655,\n",
              " 'unlik': 9297,\n",
              " 'mani': 5212,\n",
              " 'brand': 981,\n",
              " 'espresso': 2925,\n",
              " 'tri': 9082,\n",
              " 'can': 1210,\n",
              " 'illi': 4269,\n",
              " 'issimo': 4510,\n",
              " 'someth': 8104,\n",
              " 'fifti': 3190,\n",
              " 'calori': 1197,\n",
              " 'full': 3462,\n",
              " 'hit': 4097,\n",
              " 'caffein': 1180,\n",
              " 'theyll': 8850,\n",
              " 'fit': 3230,\n",
              " 'cool': 1908,\n",
              " 'sip': 7899,\n",
              " 'low': 5086,\n",
              " 'tide': 8914,\n",
              " 'rest': 7268,\n",
              " 'work': 9835,\n",
              " 'good': 3683,\n",
              " 'receiv': 7080,\n",
              " 'went': 9687,\n",
              " 'stale': 8288,\n",
              " 'within': 9811,\n",
              " 'packag': 6184,\n",
              " 'use': 9393,\n",
              " 'go': 3660,\n",
              " 'consum': 1867,\n",
              " 'real': 7051,\n",
              " 'treat': 9075,\n",
              " 'thai': 8821,\n",
              " 'yellow': 9905,\n",
              " 'bean': 663,\n",
              " 'past': 6287,\n",
              " 'pictur': 6484,\n",
              " 'sure': 8575,\n",
              " 'sent': 7703,\n",
              " 'label': 4791,\n",
              " 'total': 9014,\n",
              " 'familiar': 3088,\n",
              " 'becasu': 674,\n",
              " 'advertis': 108,\n",
              " 'photo': 6466,\n",
              " 'think': 8864,\n",
              " 'alot': 238,\n",
              " 'stuff': 8453,\n",
              " 'prepackag': 6731,\n",
              " 'heat': 4012,\n",
              " 'thing': 8862,\n",
              " 'easi': 2729,\n",
              " 'enough': 2861,\n",
              " 'veg': 9437,\n",
              " 'order': 6063,\n",
              " 'disappoint': 2445,\n",
              " 'cat': 1309,\n",
              " 'toy': 9034,\n",
              " 'liter': 5018,\n",
              " 'piec': 6486,\n",
              " 'anyth': 341,\n",
              " 'describ': 2334,\n",
              " 'turn': 9161,\n",
              " 'catnip': 1319,\n",
              " 'place': 6538,\n",
              " 'basi': 629,\n",
              " 'mallomar': 5186,\n",
              " 'star': 8302,\n",
              " 'smore': 8019,\n",
              " 'altern': 247,\n",
              " 'long': 5051,\n",
              " 'heard': 4002,\n",
              " 'start': 8310,\n",
              " 'graham': 3723,\n",
              " 'layer': 4869,\n",
              " 'marshmallow': 5265,\n",
              " 'cap': 1230,\n",
              " 'delici': 2283,\n",
              " 'theyr': 8851,\n",
              " 'temptat': 8784,\n",
              " 'keep': 4677,\n",
              " 'pop': 6639,\n",
              " 'problem': 6785,\n",
              " 'singl': 7896,\n",
              " 'doesnt': 2555,\n",
              " 'travel': 9068,\n",
              " 'consid': 1854,\n",
              " 'way': 9645,\n",
              " 'toss': 9012,\n",
              " 'around': 403,\n",
              " 'mail': 5159,\n",
              " 'up': 9365,\n",
              " 'truck': 9114,\n",
              " 'idea': 4253,\n",
              " 'half': 3895,\n",
              " 'cooki': 1906,\n",
              " 'shipment': 7796,\n",
              " 'either': 2775,\n",
              " 'broken': 1059,\n",
              " 'kind': 4713,\n",
              " 'detract': 2362,\n",
              " 'imag': 4273,\n",
              " 'small': 7989,\n",
              " 'deliveri': 2287,\n",
              " 'guy': 3868,\n",
              " 'wasnt': 9630,\n",
              " 'crumb': 2071,\n",
              " 'breakag': 995,\n",
              " 'unavoid': 9218,\n",
              " 'ship': 7794,\n",
              " 'method': 5417,\n",
              " 'shrink': 7836,\n",
              " 'wrap': 9859,\n",
              " 'crumbi': 2072,\n",
              " 'recomend': 7095,\n",
              " 'mango': 5209,\n",
              " 'dri': 2640,\n",
              " 'jasmin': 4549,\n",
              " 'pearl': 6334,\n",
              " 'tea': 8744,\n",
              " 'say': 7580,\n",
              " 'gave': 3536,\n",
              " 'tin': 8937,\n",
              " 'friend': 3424,\n",
              " 'rave': 7035,\n",
              " 'definit': 2264,\n",
              " 'explor': 3028,\n",
              " 'green': 3779,\n",
              " 'mountain': 5636,\n",
              " 'french': 3410,\n",
              " 'decaf': 2222,\n",
              " 'weak': 9649,\n",
              " 'lack': 4799,\n",
              " 'cannot': 1226,\n",
              " 'wait': 9589,\n",
              " 'gone': 3680,\n",
              " 'anoth': 323,\n",
              " 'settl': 7728,\n",
              " 'choic': 1535,\n",
              " 'planter': 6549,\n",
              " 'delux': 2289,\n",
              " 'mix': 5531,\n",
              " 'nut': 5931,\n",
              " 'negat': 5770,\n",
              " 'extrem': 3049,\n",
              " 'cant': 1228,\n",
              " 'hide': 4070,\n",
              " 'without': 9812,\n",
              " 'search': 7653,\n",
              " 'hour': 4173,\n",
              " 'reason': 7063,\n",
              " 'bag': 558,\n",
              " 'now': 5907,\n",
              " 'got': 3705,\n",
              " 'remark': 7198,\n",
              " 'perfect': 6388,\n",
              " 'incred': 4330,\n",
              " 'unfortun': 9269,\n",
              " 'descript': 2335,\n",
              " 'milo': 5475,\n",
              " 'kitchen': 4726,\n",
              " 'bought': 946,\n",
              " 'made': 5143,\n",
              " 'china': 1503,\n",
              " 'irradi': 4498,\n",
              " 'symbol': 8636,\n",
              " 'die': 2395,\n",
              " 'year': 9900,\n",
              " 'ago': 146,\n",
              " 'feed': 3145,\n",
              " 'unawar': 9219,\n",
              " 'recommend': 7098,\n",
              " 'staci': 8277,\n",
              " 'case': 1298,\n",
              " 'groceri': 3807,\n",
              " 'store': 8394,\n",
              " 'signific': 7865,\n",
              " 'came': 1201,\n",
              " 'quick': 6960,\n",
              " 'hous': 4174,\n",
              " 'pleas': 6561,\n",
              " 'amount': 284,\n",
              " 'belt': 717,\n",
              " 'candi': 1215,\n",
              " 'shop': 7811,\n",
              " 'sour': 8141,\n",
              " 'believ': 708,\n",
              " 'chose': 1552,\n",
              " 'thought': 8878,\n",
              " 'two': 9185,\n",
              " 'pack': 6183,\n",
              " 'doubl': 2597,\n",
              " 'attent': 481,\n",
              " 'whatev': 9702,\n",
              " 'put': 6928,\n",
              " 'fanci': 3092,\n",
              " 'feast': 3133,\n",
              " 'food': 3325,\n",
              " 'goe': 3670,\n",
              " 'bonker': 906,\n",
              " 'must': 5699,\n",
              " 'forgot': 3350,\n",
              " 'ten': 8785,\n",
              " 'old': 6012,\n",
              " 'nephew': 5780,\n",
              " 'birthday': 798,\n",
              " 'cake': 1187,\n",
              " 'dip': 2432,\n",
              " 'didnt': 2394,\n",
              " 'replac': 7224,\n",
              " 'charact': 1409,\n",
              " 'never': 5803,\n",
              " 'contact': 1869,\n",
              " 'forward': 3366,\n",
              " 'feel': 3148,\n",
              " 'judg': 4613,\n",
              " 'fact': 3066,\n",
              " 'alway': 258,\n",
              " 'nose': 5883,\n",
              " 'took': 8989,\n",
              " 'bite': 814,\n",
              " 'chocol': 1527,\n",
              " 'oreo': 6070,\n",
              " 'taken': 8673,\n",
              " 'aback': 1,\n",
              " 'expect': 3013,\n",
              " 'chunk': 1569,\n",
              " 'peanut': 6329,\n",
              " 'butter': 1149,\n",
              " 'creme': 2031,\n",
              " 'kosher': 4768,\n",
              " 'bagel': 559,\n",
              " 'twice': 9173,\n",
              " 'day': 2197,\n",
              " 'still': 8364,\n",
              " 'quit': 6970,\n",
              " 'chewi': 1477,\n",
              " 'second': 7660,\n",
              " 'delay': 2274,\n",
              " 'plus': 6581,\n",
              " 'werent': 9688,\n",
              " 'new': 5805,\n",
              " 'includ': 4319,\n",
              " 'compens': 1772,\n",
              " 'thank': 8823,\n",
              " 'enclos': 2834,\n",
              " 'card': 1258,\n",
              " 'mark': 5251,\n",
              " 'locat': 5039,\n",
              " 'map': 5226,\n",
              " 'offer': 5996,\n",
              " 'varieti': 9430,\n",
              " 'poppi': 6643,\n",
              " 'urg': 9383,\n",
              " 'seller': 7686,\n",
              " 'perhap': 6395,\n",
              " 'vacuum': 9409,\n",
              " 'sinc': 7890,\n",
              " 'opinion': 6044,\n",
              " 'mayb': 5315,\n",
              " 'intend': 4445,\n",
              " 'free': 3403,\n",
              " 'reduct': 7130,\n",
              " 'previous': 6751,\n",
              " 'wheat': 9704,\n",
              " 'corn': 1931,\n",
              " 'gluten': 3642,\n",
              " 'brewer': 1014,\n",
              " 'garlic': 3517,\n",
              " 'roll': 7394,\n",
              " 'whole': 9747,\n",
              " 'grain': 3725,\n",
              " 'aroma': 401,\n",
              " 'jelli': 4561,\n",
              " 'hard': 3938,\n",
              " 'set': 7727,\n",
              " 'tabl': 8650,\n",
              " 'garden': 3512,\n",
              " 'brunch': 1076,\n",
              " 'wed': 9664,\n",
              " 'town': 9030,\n",
              " 'terribl': 8805,\n",
              " 'enrich': 2863,\n",
              " 'shelter': 7770,\n",
              " 'cut': 2126,\n",
              " 'tie': 8916,\n",
              " 'togeth': 8971,\n",
              " 'spray': 8243,\n",
              " 'crazi': 2014,\n",
              " 'aloha': 233,\n",
              " 'soy': 8150,\n",
              " 'sauc': 7562,\n",
              " 'fabul': 3058,\n",
              " 'overpow': 6148,\n",
              " 'sodium': 8077,\n",
              " 'name': 5723,\n",
              " 'sweet': 8611,\n",
              " 'caramel': 1251,\n",
              " 'excel': 2988,\n",
              " 'substitut': 8486,\n",
              " 'mild': 5459,\n",
              " 'marmalad': 5257,\n",
              " 'read': 7047,\n",
              " 'least': 4889,\n",
              " 'bitter': 817,\n",
              " 'peel': 6347,\n",
              " 'come': 1745,\n",
              " 'list': 5013,\n",
              " 'balanc': 571,\n",
              " 'thick': 8854,\n",
              " 'everyon': 2966,\n",
              " 'specialti': 8180,\n",
              " 'isnt': 4507,\n",
              " 'dalfour': 2153,\n",
              " 'repres': 7230,\n",
              " 'type': 9187,\n",
              " 'antioxid': 331,\n",
              " 'rather': 7031,\n",
              " 'neither': 5775,\n",
              " 'daughter': 2190,\n",
              " 'nor': 5873,\n",
              " 'remaind': 7196,\n",
              " 'school': 7609,\n",
              " 'student': 8449,\n",
              " 'share': 7754,\n",
              " 'among': 280,\n",
              " 'teenag': 8767,\n",
              " 'review': 7300,\n",
              " 'popcorn': 6641,\n",
              " 'top': 8999,\n",
              " 'movi': 5647,\n",
              " 'miss': 5518,\n",
              " 'produt': 6799,\n",
              " 'tablespoon': 8651,\n",
              " 'end': 2838,\n",
              " 'wast': 9632,\n",
              " 'tube': 9136,\n",
              " 'melt': 5378,\n",
              " 'gooey': 3687,\n",
              " 'fruit': 3448,\n",
              " 'couldnt': 1963,\n",
              " 'hate': 3968,\n",
              " 'blame': 830,\n",
              " 'gummi': 3857,\n",
              " 'reorder': 7216,\n",
              " 'mommi': 5570,\n",
              " 'grab': 3716,\n",
              " 'let': 4936,\n",
              " 'arent': 389,\n",
              " 'cheap': 1427,\n",
              " 'babi': 539,\n",
              " 'know': 4742,\n",
              " 'worri': 9843,\n",
              " 'choke': 1536,\n",
              " 'fast': 3109,\n",
              " 'other': 6097,\n",
              " 'pound': 6684,\n",
              " 'pleasant': 6562,\n",
              " 'much': 5658,\n",
              " 'part': 6271,\n",
              " 'adjust': 91,\n",
              " 'bottl': 941,\n",
              " 'easili': 2732,\n",
              " 'pocket': 6586,\n",
              " 'notic': 5894,\n",
              " 'diet': 2398,\n",
              " 'etc': 2937,\n",
              " 'usual': 9398,\n",
              " 'experi': 3018,\n",
              " 'immedi': 4280,\n",
              " 'given': 3603,\n",
              " 'soda': 8074,\n",
              " 'realiz': 7056,\n",
              " 'sugar': 8513,\n",
              " 'depend': 2316,\n",
              " 'person': 6419,\n",
              " 'fruiti': 3450,\n",
              " 'anyon': 340,\n",
              " 'spend': 8195,\n",
              " 'money': 5575,\n",
              " 'save': 7573,\n",
              " 'cube': 2088,\n",
              " 'disappear': 2444,\n",
              " 'shelv': 7772,\n",
              " 'far': 3099,\n",
              " 'larg': 4834,\n",
              " 'citi': 1596,\n",
              " 'yay': 9897,\n",
              " 'roland': 7392,\n",
              " 'similar': 7876,\n",
              " 'europ': 2947,\n",
              " 'white': 9738,\n",
              " 'brown': 1069,\n",
              " 'crush': 2080,\n",
              " 'tomato': 8978,\n",
              " 'textur': 8819,\n",
              " 'cook': 1903,\n",
              " 'wonder': 9821,\n",
              " 'sale': 7506,\n",
              " 'januari': 4545,\n",
              " 'hot': 4166,\n",
              " 'enjoy': 2857,\n",
              " 'right': 7336,\n",
              " 'alreadi': 243,\n",
              " 'stop': 8391,\n",
              " 'core': 1926,\n",
              " 'formula': 3359,\n",
              " 'thorough': 8876,\n",
              " 'pick': 6477,\n",
              " 'leav': 4892,\n",
              " 'wet': 9696,\n",
              " 'protein': 6844,\n",
              " 'stapl': 8301,\n",
              " 'ate': 468,\n",
              " 'later': 4845,\n",
              " 'clear': 1631,\n",
              " 'fed': 3141,\n",
              " 'threw': 8888,\n",
              " 'higher': 4074,\n",
              " 'acclim': 39,\n",
              " 'option': 6053,\n",
              " 'shed': 7763,\n",
              " 'sad': 7480,\n",
              " 'wont': 9824,\n",
              " 'agre': 149,\n",
              " 'chanc': 1399,\n",
              " 'wrong': 9871,\n",
              " 'what': 9701,\n",
              " 'caus': 1325,\n",
              " 'speak': 8173,\n",
              " 'ill': 4267,\n",
              " 'updat': 9368,\n",
              " 'suffer': 8509,\n",
              " 'acid': 58,\n",
              " 'reflux': 7139,\n",
              " 'human': 4201,\n",
              " 'rich': 7321,\n",
              " 'flare': 3244,\n",
              " 'awesom': 529,\n",
              " 'wife': 9768,\n",
              " 'lap': 4828,\n",
              " 'band': 587,\n",
              " 'surgeri': 8579,\n",
              " 'meal': 5330,\n",
              " 'greek': 3778,\n",
              " 'yogurt': 9924,\n",
              " 'hidden': 4069,\n",
              " 'valley': 9414,\n",
              " 'ranch': 7010,\n",
              " 'salad': 7502,\n",
              " 'dress': 2638,\n",
              " 'item': 4519,\n",
              " 'sprout': 8256,\n",
              " 'stuck': 8448,\n",
              " 'addit': 83,\n",
              " 'packet': 6187,\n",
              " 'creami': 2017,\n",
              " 'base': 625,\n",
              " 'inform': 4381,\n",
              " 'cartridg': 1292,\n",
              " 'call': 1193,\n",
              " 'told': 8974,\n",
              " 'care': 1266,\n",
              " 'hodgson': 4104,\n",
              " 'result': 7278,\n",
              " 'loaf': 5033,\n",
              " 'fall': 3082,\n",
              " 'apart': 345,\n",
              " 'honey': 4129,\n",
              " 'bad': 556,\n",
              " 'risk': 7352,\n",
              " 'arriv': 407,\n",
              " 'glad': 3610,\n",
              " 'holiday': 4111,\n",
              " 'finger': 3211,\n",
              " 'solid': 8090,\n",
              " 'bulli': 1110,\n",
              " 'skin': 7924,\n",
              " 'cartilag': 1289,\n",
              " 'wherea': 9713,\n",
              " 'normal': 5877,\n",
              " 'pretti': 6745,\n",
              " 'crunchier': 2078,\n",
              " 'pig': 6490,\n",
              " 'ear': 2717,\n",
              " 'oppos': 6046,\n",
              " 'month': 5595,\n",
              " 'belgian': 705,\n",
              " 'shepherd': 7775,\n",
              " 'huge': 4192,\n",
              " 'averag': 513,\n",
              " 'hell': 4037,\n",
              " 'finish': 3215,\n",
              " 'away': 527,\n",
              " 'minut': 5496,\n",
              " 'chew': 1473,\n",
              " 'braid': 975,\n",
              " 'twist': 9179,\n",
              " 'longer': 5052,\n",
              " 'home': 4118,\n",
              " 'night': 5833,\n",
              " 'entir': 2874,\n",
              " 'inch': 4315,\n",
              " 'less': 4931,\n",
              " 'anywher': 344,\n",
              " 'stunk': 8459,\n",
              " 'high': 4073,\n",
              " 'polici': 6605,\n",
              " 'suck': 8500,\n",
              " 'restock': 7272,\n",
              " 'fee': 3144,\n",
              " 'hope': 4145,\n",
              " 'sick': 7850,\n",
              " 'desper': 2343,\n",
              " 'frustrat': 3452,\n",
              " 'throw': 8897,\n",
              " 'burnt': 1135,\n",
              " 'various': 9431,\n",
              " 'expens': 3016,\n",
              " 'kona': 4755,\n",
              " 'sore': 8129,\n",
              " 'cri': 2037,\n",
              " 'thrown': 8899,\n",
              " 'yet': 9915,\n",
              " 'grew': 3786,\n",
              " 'recent': 7081,\n",
              " 'near': 5750,\n",
              " 'matter': 5302,\n",
              " 'research': 7244,\n",
              " 'today': 8964,\n",
              " 'decid': 2233,\n",
              " 'roast': 7368,\n",
              " 'merchandis': 5394,\n",
              " 'quantiti': 6945,\n",
              " 'process': 6789,\n",
              " 'pride': 6759,\n",
              " 'truli': 9123,\n",
              " 'ridicul': 7335,\n",
              " 'version': 9481,\n",
              " 'load': 5032,\n",
              " 'artifici': 423,\n",
              " 'whove': 9760,\n",
              " 'eaten': 2738,\n",
              " 'bread': 989,\n",
              " 'hand': 3912,\n",
              " 'machin': 5138,\n",
              " 'succumb': 8498,\n",
              " 'mass': 5284,\n",
              " 'inferior': 4370,\n",
              " 'began': 693,\n",
              " 'decad': 2221,\n",
              " 'habit': 3878,\n",
              " 'spit': 8216,\n",
              " 'said': 7498,\n",
              " 'macaroni': 5133,\n",
              " 'healthfood': 3994,\n",
              " 'dish': 2479,\n",
              " 'mac': 5128,\n",
              " 'cold': 1716,\n",
              " 'loos': 5061,\n",
              " 'shape': 7752,\n",
              " 'dislik': 2489,\n",
              " 'pasta': 6288,\n",
              " 'goo': 3682,\n",
              " 'water': 9635,\n",
              " 'rins': 7343,\n",
              " 'ad': 77,\n",
              " 'honest': 4127,\n",
              " 'deal': 2205,\n",
              " 'goug': 3710,\n",
              " 'exceed': 2987,\n",
              " 'bug': 1096,\n",
              " 'afraid': 130,\n",
              " 'flour': 3291,\n",
              " 'clean': 1625,\n",
              " 'though': 8877,\n",
              " 'left': 4905,\n",
              " 'porch': 6649,\n",
              " 'smaller': 7990,\n",
              " 'subtl': 8488,\n",
              " 'gotten': 3708,\n",
              " 'bake': 567,\n",
              " 'ultim': 9202,\n",
              " 'flea': 3263,\n",
              " 'live': 5025,\n",
              " 'california': 1192,\n",
              " 'sand': 7532,\n",
              " 'patio': 6305,\n",
              " 'screen': 7638,\n",
              " 'four': 3374,\n",
              " 'move': 5645,\n",
              " 'continu': 1881,\n",
              " 'trap': 9063,\n",
              " 'child': 1494,\n",
              " 'felt': 3154,\n",
              " 'avoid': 519,\n",
              " 'caught': 1323,\n",
              " 'occasion': 5978,\n",
              " 'random': 7015,\n",
              " 'creepi': 2029,\n",
              " 'attract': 486,\n",
              " 'tuck': 9137,\n",
              " 'sight': 7860,\n",
              " 'corner': 1933,\n",
              " 'pet': 6432,\n",
              " 'curious': 2112,\n",
              " 'refil': 7135,\n",
              " 'weve': 9700,\n",
              " 'bulb': 1102,\n",
              " 'regular': 7167,\n",
              " 'light': 4968,\n",
              " 'led': 4898,\n",
              " 'kashi': 4661,\n",
              " 'fat': 3112,\n",
              " 'ingredi': 4387,\n",
              " 'recogniz': 7092,\n",
              " 'soft': 8079,\n",
              " 'unusu': 9357,\n",
              " 'cheesecak': 1449,\n",
              " 'hazelnut': 3983,\n",
              " 'serv': 7722,\n",
              " 'guess': 3840,\n",
              " 'virtual': 9534,\n",
              " 'ran': 7009,\n",
              " 'compet': 1773,\n",
              " 'red': 7117,\n",
              " 'curri': 2116,\n",
              " 'shrimp': 7835,\n",
              " 'entre': 2879,\n",
              " 'simmer': 7878,\n",
              " 'meat': 5340,\n",
              " 'veget': 9443,\n",
              " 'lite': 5017,\n",
              " 'milk': 5465,\n",
              " 'creat': 2020,\n",
              " 'word': 9833,\n",
              " 'substitu': 8485,\n",
              " 'lbs': 4871,\n",
              " 'bland': 833,\n",
              " 'patak': 6296,\n",
              " 'messag': 5409,\n",
              " 'talk': 8676,\n",
              " 'indian': 4340,\n",
              " 'organ': 6072,\n",
              " 'week': 9670,\n",
              " 'friday': 3422,\n",
              " 'morn': 5609,\n",
              " 'board': 878,\n",
              " 'posit': 6662,\n",
              " 'feedback': 3146,\n",
              " 'starbuck': 8303,\n",
              " 'cafe': 1177,\n",
              " 'verona': 9477,\n",
              " 'famous': 3089,\n",
              " 'show': 7831,\n",
              " 'bulk': 1105,\n",
              " 'help': 4042,\n",
              " 'econom': 2748,\n",
              " 'supermarket': 8560,\n",
              " 'instead': 4430,\n",
              " 'inflat': 4376,\n",
              " 'drawer': 2631,\n",
              " 'sit': 7903,\n",
              " 'keurig': 4697,\n",
              " 'coffer': 1705,\n",
              " 'ecstat': 2750,\n",
              " 'hold': 4107,\n",
              " 'everyth': 2967,\n",
              " 'energi': 2849,\n",
              " 'foami': 3307,\n",
              " 'carbon': 1256,\n",
              " 'vitamin': 9549,\n",
              " 'cheaper': 1429,\n",
              " 'easier': 2730,\n",
              " 'harvest': 3959,\n",
              " 'seed': 7668,\n",
              " 'final': 3205,\n",
              " 'tripl': 9100,\n",
              " 'exclus': 2995,\n",
              " 'begun': 697,\n",
              " 'spoil': 8226,\n",
              " 'pie': 6485,\n",
              " 'fill': 3197,\n",
              " 'oregon': 6069,\n",
              " 'rais': 6999,\n",
              " 'guarante': 3835,\n",
              " 'ginger': 3595,\n",
              " 'ale': 194,\n",
              " 'abl': 11,\n",
              " 'toler': 8975,\n",
              " 'beverag': 755,\n",
              " 'bubbl': 1083,\n",
              " 'bourbon': 955,\n",
              " 'laugh': 4852,\n",
              " 'properti': 6831,\n",
              " 'flu': 3296,\n",
              " 'vernor': 9476,\n",
              " 'pale': 6203,\n",
              " 'comparison': 1767,\n",
              " 'proceed': 6788,\n",
              " 'husband': 4222,\n",
              " 'hes': 4059,\n",
              " 'shown': 7833,\n",
              " 'compost': 1791,\n",
              " 'bin': 782,\n",
              " 'tip': 8947,\n",
              " 'hat': 3966,\n",
              " 'brilliant': 1029,\n",
              " 'gold': 3674,\n",
              " 'spici': 8201,\n",
              " 'amber': 266,\n",
              " 'shake': 7743,\n",
              " 'fist': 3229,\n",
              " 'strict': 8427,\n",
              " 'distribut': 2527,\n",
              " 'admit': 95,\n",
              " 'lone': 5050,\n",
              " 'respons': 7267,\n",
              " 'somewhat': 8106,\n",
              " 'imagin': 4274,\n",
              " 'employe': 2823,\n",
              " 'island': 4506,\n",
              " 'paid': 6191,\n",
              " 'nowher': 5909,\n",
              " 'phenomen': 6452,\n",
              " 'retail': 7281,\n",
              " 'rival': 7359,\n",
              " 'accept': 34,\n",
              " 'due': 2680,\n",
              " 'specif': 8181,\n",
              " 'albeit': 188,\n",
              " 'medium': 5358,\n",
              " 'favor': 3126,\n",
              " 'hesit': 4060,\n",
              " 'refund': 7148,\n",
              " 'true': 9116,\n",
              " 'claim': 1605,\n",
              " 'satisfact': 7557,\n",
              " 'tapioca': 8706,\n",
              " 'extra': 3045,\n",
              " 'generic': 3550,\n",
              " 'spice': 8199,\n",
              " 'notat': 5889,\n",
              " 'individu': 4349,\n",
              " 'fri': 3420,\n",
              " 'potato': 6673,\n",
              " 'hubbi': 4188,\n",
              " 'fool': 3328,\n",
              " 'bar': 596,\n",
              " 'contain': 1870,\n",
              " 'partial': 6275,\n",
              " 'hydrogen': 4230,\n",
              " 'oil': 6006,\n",
              " 'seven': 7730,\n",
              " 'gram': 3728,\n",
              " 'becom': 676,\n",
              " 'english': 2855,\n",
              " 'breakfast': 998,\n",
              " 'howev': 4183,\n",
              " 'rural': 7453,\n",
              " 'happi': 3931,\n",
              " 'job': 4587,\n",
              " 'struck': 8441,\n",
              " 'instruct': 4433,\n",
              " 'indic': 4342,\n",
              " 'powder': 6689,\n",
              " 'patti': 6309,\n",
              " 'conveni': 1892,\n",
              " 'expir': 3022,\n",
              " 'date': 2189,\n",
              " 'complain': 1776,\n",
              " 'happen': 3929,\n",
              " 'chestnut': 1472,\n",
              " 'costco': 1954,\n",
              " 'rememb': 7200,\n",
              " 'stock': 8375,\n",
              " 'onlin': 6032,\n",
              " 'figur': 3193,\n",
              " 'term': 8801,\n",
              " 'relationship': 7180,\n",
              " 'prompt': 6821,\n",
              " 'tore': 9003,\n",
              " 'eager': 2715,\n",
              " 'saw': 7578,\n",
              " 'mushi': 5693,\n",
              " 'reluct': 7194,\n",
              " 'rotten': 7421,\n",
              " 'affect': 122,\n",
              " 'spirit': 8214,\n",
              " 'sold': 8087,\n",
              " 'knock': 4739,\n",
              " 'serious': 7718,\n",
              " 'spread': 8245,\n",
              " 'banana': 586,\n",
              " 'pretzel': 6748,\n",
              " 'cracker': 1995,\n",
              " 'sweeter': 8613,\n",
              " 'consist': 1856,\n",
              " 'per': 6380,\n",
              " 'check': 1436,\n",
              " 'ting': 8939,\n",
              " 'kitten': 4728,\n",
              " 'troubl': 9111,\n",
              " 'diarrhea': 2389,\n",
              " 'medic': 5352,\n",
              " 'allergi': 222,\n",
              " 'sensit': 7701,\n",
              " 'gradual': 3721,\n",
              " 'switch': 8631,\n",
              " 'success': 8496,\n",
              " 'slow': 7979,\n",
              " 'eventu': 2959,\n",
              " 'plan': 6542,\n",
              " 'nutrit': 5945,\n",
              " 'content': 1875,\n",
              " 'premium': 6728,\n",
              " 'vet': 9486,\n",
              " 'coat': 1681,\n",
              " 'shini': 7792,\n",
              " 'weight': 9679,\n",
              " 'picki': 6478,\n",
              " 'limit': 4986,\n",
              " 'thrill': 8891,\n",
              " 'kitti': 4729,\n",
              " 'charg': 1414,\n",
              " 'deliv': 2286,\n",
              " 'door': 2581,\n",
              " 'ask': 440,\n",
              " 'unpop': 9315,\n",
              " 'eden': 2753,\n",
              " 'gourmet': 3712,\n",
              " 'luv': 5122,\n",
              " 'bistro': 811,\n",
              " 'measur': 5339,\n",
              " 'state': 8317,\n",
              " 'usa': 9387,\n",
              " 'appar': 351,\n",
              " ...}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GA4HARNBAzKW"
      },
      "source": [
        "# transformation using tf-idf\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=10000)\n",
        "tfidf_vectorizer.fit(X_train)\n",
        "\n",
        "# transform\n",
        "tfidf_X_train = tfidf_vectorizer.transform(X_train)\n",
        "tfidf_X_test = tfidf_vectorizer.transform(X_test)\n",
        "tfidf_X1_train = tfidf_vectorizer.transform(X1_train)\n",
        "tfidf_X1_test = tfidf_vectorizer.transform(X1_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0kfQ4Q7CiV6",
        "outputId": "76f7cbd5-95df-46e2-cf0d-94310e63d6f6"
      },
      "source": [
        "tfidf_vectorizer.vocabulary_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dog': 2557,\n",
              " 'love': 5084,\n",
              " 'not': 5887,\n",
              " 'stand': 8294,\n",
              " 'jerki': 4570,\n",
              " 'best': 746,\n",
              " 'ive': 4524,\n",
              " 'ever': 2960,\n",
              " 'great': 3772,\n",
              " 'also': 245,\n",
              " 'appl': 358,\n",
              " 'slice': 7957,\n",
              " 'time': 8932,\n",
              " 'wish': 9804,\n",
              " 'could': 1962,\n",
              " 'find': 3207,\n",
              " 'local': 5038,\n",
              " 'chain': 1389,\n",
              " 'seem': 7671,\n",
              " 'carri': 1284,\n",
              " 'sever': 7733,\n",
              " 'worth': 9847,\n",
              " 'send': 7693,\n",
              " 'get': 3573,\n",
              " 'dont': 2577,\n",
              " 'want': 9609,\n",
              " 'tag': 8661,\n",
              " 'amazon': 264,\n",
              " 'there': 8841,\n",
              " 'run': 7447,\n",
              " 'might': 5451,\n",
              " 'avail': 510,\n",
              " 'next': 5816,\n",
              " 'especi': 2922,\n",
              " 'coffe': 1701,\n",
              " 'provid': 6854,\n",
              " 'aromat': 402,\n",
              " 'scent': 7601,\n",
              " 'air': 164,\n",
              " 'brew': 1013,\n",
              " 'fresh': 3416,\n",
              " 'well': 9684,\n",
              " 'ground': 3814,\n",
              " 'size': 7912,\n",
              " 'nice': 5822,\n",
              " 'make': 5175,\n",
              " 'cup': 2102,\n",
              " 'flavor': 3255,\n",
              " 'bit': 813,\n",
              " 'lighter': 4970,\n",
              " 'side': 7853,\n",
              " 'may': 5312,\n",
              " 'favorit': 3127,\n",
              " 'your': 9937,\n",
              " 'realli': 7058,\n",
              " 'dark': 2174,\n",
              " 'wolfgang': 9818,\n",
              " 'puck': 6869,\n",
              " 'sign': 7861,\n",
              " 'recip': 7085,\n",
              " 'wooden': 9828,\n",
              " 'tongu': 8984,\n",
              " 'doctor': 2550,\n",
              " 'offic': 5997,\n",
              " 'thin': 8861,\n",
              " 'like': 4974,\n",
              " 'chunki': 1570,\n",
              " 'tap': 8701,\n",
              " 'even': 2955,\n",
              " 'take': 8672,\n",
              " 'tini': 8941,\n",
              " 'dumpl': 2692,\n",
              " 'back': 546,\n",
              " 'gotta': 3707,\n",
              " 'gift': 3586,\n",
              " 'coupl': 1975,\n",
              " 'drink': 2645,\n",
              " 'lot': 5074,\n",
              " 'select': 7682,\n",
              " 'probabl': 6781,\n",
              " 'would': 9853,\n",
              " 'buy': 1158,\n",
              " 'product': 6797,\n",
              " 'return': 7288,\n",
              " 'give': 3602,\n",
              " 'certif': 1379,\n",
              " 'smell': 8002,\n",
              " 'fish': 3226,\n",
              " 'strong': 8438,\n",
              " 'crumbl': 2073,\n",
              " 'eat': 2736,\n",
              " 'chicken': 1485,\n",
              " 'allerg': 220,\n",
              " 'seen': 7672,\n",
              " 'beef': 683,\n",
              " 'veggi': 9446,\n",
              " 'stick': 8355,\n",
              " 'littl': 5024,\n",
              " 'price': 6754,\n",
              " 'purchas': 6906,\n",
              " 'better': 752,\n",
              " 'need': 5765,\n",
              " 'actual': 74,\n",
              " 'possess': 6663,\n",
              " 'almost': 231,\n",
              " 'exact': 2981,\n",
              " 'vanilla': 9420,\n",
              " 'tile': 8928,\n",
              " 'look': 5056,\n",
              " 'healthi': 3995,\n",
              " 'oatmeal': 5960,\n",
              " 'that': 8827,\n",
              " 'natur': 5742,\n",
              " 'path': 6300,\n",
              " 'tasti': 8727,\n",
              " 'one': 6028,\n",
              " 'compar': 1766,\n",
              " 'surpris': 8583,\n",
              " 'open': 6042,\n",
              " 'box': 961,\n",
              " 'found': 3370,\n",
              " 'proper': 6830,\n",
              " 'spill': 8208,\n",
              " 'insid': 4419,\n",
              " 'obvious': 5976,\n",
              " 'first': 3224,\n",
              " 'last': 4842,\n",
              " 'although': 249,\n",
              " 'tast': 8722,\n",
              " 'pay': 6315,\n",
              " 'trash': 9065,\n",
              " 'poor': 6636,\n",
              " 'where': 9712,\n",
              " 'qualiti': 6942,\n",
              " 'control': 1889,\n",
              " 'big': 771,\n",
              " 'fan': 3090,\n",
              " 'coconut': 1694,\n",
              " 'prefer': 6719,\n",
              " 'vita': 9544,\n",
              " 'slight': 7962,\n",
              " 'inconsist': 4325,\n",
              " 'across': 68,\n",
              " 'differ': 2403,\n",
              " 'issu': 4511,\n",
              " 'compani': 1764,\n",
              " 'orang': 6055,\n",
              " 'juic': 4617,\n",
              " 'oxygen': 6175,\n",
              " 'add': 81,\n",
              " 'chemic': 1459,\n",
              " 'produc': 6796,\n",
              " 'absolut': 19,\n",
              " 'suppos': 8570,\n",
              " 'peopl': 6368,\n",
              " 'imposs': 4296,\n",
              " 'everi': 2961,\n",
              " 'batch': 640,\n",
              " 'unless': 9296,\n",
              " 'modifi': 5549,\n",
              " 'coco': 1692,\n",
              " 'closest': 1655,\n",
              " 'unlik': 9297,\n",
              " 'mani': 5212,\n",
              " 'brand': 981,\n",
              " 'espresso': 2925,\n",
              " 'tri': 9082,\n",
              " 'can': 1210,\n",
              " 'illi': 4269,\n",
              " 'issimo': 4510,\n",
              " 'someth': 8104,\n",
              " 'fifti': 3190,\n",
              " 'calori': 1197,\n",
              " 'full': 3462,\n",
              " 'hit': 4097,\n",
              " 'caffein': 1180,\n",
              " 'theyll': 8850,\n",
              " 'fit': 3230,\n",
              " 'cool': 1908,\n",
              " 'sip': 7899,\n",
              " 'low': 5086,\n",
              " 'tide': 8914,\n",
              " 'rest': 7268,\n",
              " 'work': 9835,\n",
              " 'good': 3683,\n",
              " 'receiv': 7080,\n",
              " 'went': 9687,\n",
              " 'stale': 8288,\n",
              " 'within': 9811,\n",
              " 'packag': 6184,\n",
              " 'use': 9393,\n",
              " 'go': 3660,\n",
              " 'consum': 1867,\n",
              " 'real': 7051,\n",
              " 'treat': 9075,\n",
              " 'thai': 8821,\n",
              " 'yellow': 9905,\n",
              " 'bean': 663,\n",
              " 'past': 6287,\n",
              " 'pictur': 6484,\n",
              " 'sure': 8575,\n",
              " 'sent': 7703,\n",
              " 'label': 4791,\n",
              " 'total': 9014,\n",
              " 'familiar': 3088,\n",
              " 'becasu': 674,\n",
              " 'advertis': 108,\n",
              " 'photo': 6466,\n",
              " 'think': 8864,\n",
              " 'alot': 238,\n",
              " 'stuff': 8453,\n",
              " 'prepackag': 6731,\n",
              " 'heat': 4012,\n",
              " 'thing': 8862,\n",
              " 'easi': 2729,\n",
              " 'enough': 2861,\n",
              " 'veg': 9437,\n",
              " 'order': 6063,\n",
              " 'disappoint': 2445,\n",
              " 'cat': 1309,\n",
              " 'toy': 9034,\n",
              " 'liter': 5018,\n",
              " 'piec': 6486,\n",
              " 'anyth': 341,\n",
              " 'describ': 2334,\n",
              " 'turn': 9161,\n",
              " 'catnip': 1319,\n",
              " 'place': 6538,\n",
              " 'basi': 629,\n",
              " 'mallomar': 5186,\n",
              " 'star': 8302,\n",
              " 'smore': 8019,\n",
              " 'altern': 247,\n",
              " 'long': 5051,\n",
              " 'heard': 4002,\n",
              " 'start': 8310,\n",
              " 'graham': 3723,\n",
              " 'layer': 4869,\n",
              " 'marshmallow': 5265,\n",
              " 'cap': 1230,\n",
              " 'delici': 2283,\n",
              " 'theyr': 8851,\n",
              " 'temptat': 8784,\n",
              " 'keep': 4677,\n",
              " 'pop': 6639,\n",
              " 'problem': 6785,\n",
              " 'singl': 7896,\n",
              " 'doesnt': 2555,\n",
              " 'travel': 9068,\n",
              " 'consid': 1854,\n",
              " 'way': 9645,\n",
              " 'toss': 9012,\n",
              " 'around': 403,\n",
              " 'mail': 5159,\n",
              " 'up': 9365,\n",
              " 'truck': 9114,\n",
              " 'idea': 4253,\n",
              " 'half': 3895,\n",
              " 'cooki': 1906,\n",
              " 'shipment': 7796,\n",
              " 'either': 2775,\n",
              " 'broken': 1059,\n",
              " 'kind': 4713,\n",
              " 'detract': 2362,\n",
              " 'imag': 4273,\n",
              " 'small': 7989,\n",
              " 'deliveri': 2287,\n",
              " 'guy': 3868,\n",
              " 'wasnt': 9630,\n",
              " 'crumb': 2071,\n",
              " 'breakag': 995,\n",
              " 'unavoid': 9218,\n",
              " 'ship': 7794,\n",
              " 'method': 5417,\n",
              " 'shrink': 7836,\n",
              " 'wrap': 9859,\n",
              " 'crumbi': 2072,\n",
              " 'recomend': 7095,\n",
              " 'mango': 5209,\n",
              " 'dri': 2640,\n",
              " 'jasmin': 4549,\n",
              " 'pearl': 6334,\n",
              " 'tea': 8744,\n",
              " 'say': 7580,\n",
              " 'gave': 3536,\n",
              " 'tin': 8937,\n",
              " 'friend': 3424,\n",
              " 'rave': 7035,\n",
              " 'definit': 2264,\n",
              " 'explor': 3028,\n",
              " 'green': 3779,\n",
              " 'mountain': 5636,\n",
              " 'french': 3410,\n",
              " 'decaf': 2222,\n",
              " 'weak': 9649,\n",
              " 'lack': 4799,\n",
              " 'cannot': 1226,\n",
              " 'wait': 9589,\n",
              " 'gone': 3680,\n",
              " 'anoth': 323,\n",
              " 'settl': 7728,\n",
              " 'choic': 1535,\n",
              " 'planter': 6549,\n",
              " 'delux': 2289,\n",
              " 'mix': 5531,\n",
              " 'nut': 5931,\n",
              " 'negat': 5770,\n",
              " 'extrem': 3049,\n",
              " 'cant': 1228,\n",
              " 'hide': 4070,\n",
              " 'without': 9812,\n",
              " 'search': 7653,\n",
              " 'hour': 4173,\n",
              " 'reason': 7063,\n",
              " 'bag': 558,\n",
              " 'now': 5907,\n",
              " 'got': 3705,\n",
              " 'remark': 7198,\n",
              " 'perfect': 6388,\n",
              " 'incred': 4330,\n",
              " 'unfortun': 9269,\n",
              " 'descript': 2335,\n",
              " 'milo': 5475,\n",
              " 'kitchen': 4726,\n",
              " 'bought': 946,\n",
              " 'made': 5143,\n",
              " 'china': 1503,\n",
              " 'irradi': 4498,\n",
              " 'symbol': 8636,\n",
              " 'die': 2395,\n",
              " 'year': 9900,\n",
              " 'ago': 146,\n",
              " 'feed': 3145,\n",
              " 'unawar': 9219,\n",
              " 'recommend': 7098,\n",
              " 'staci': 8277,\n",
              " 'case': 1298,\n",
              " 'groceri': 3807,\n",
              " 'store': 8394,\n",
              " 'signific': 7865,\n",
              " 'came': 1201,\n",
              " 'quick': 6960,\n",
              " 'hous': 4174,\n",
              " 'pleas': 6561,\n",
              " 'amount': 284,\n",
              " 'belt': 717,\n",
              " 'candi': 1215,\n",
              " 'shop': 7811,\n",
              " 'sour': 8141,\n",
              " 'believ': 708,\n",
              " 'chose': 1552,\n",
              " 'thought': 8878,\n",
              " 'two': 9185,\n",
              " 'pack': 6183,\n",
              " 'doubl': 2597,\n",
              " 'attent': 481,\n",
              " 'whatev': 9702,\n",
              " 'put': 6928,\n",
              " 'fanci': 3092,\n",
              " 'feast': 3133,\n",
              " 'food': 3325,\n",
              " 'goe': 3670,\n",
              " 'bonker': 906,\n",
              " 'must': 5699,\n",
              " 'forgot': 3350,\n",
              " 'ten': 8785,\n",
              " 'old': 6012,\n",
              " 'nephew': 5780,\n",
              " 'birthday': 798,\n",
              " 'cake': 1187,\n",
              " 'dip': 2432,\n",
              " 'didnt': 2394,\n",
              " 'replac': 7224,\n",
              " 'charact': 1409,\n",
              " 'never': 5803,\n",
              " 'contact': 1869,\n",
              " 'forward': 3366,\n",
              " 'feel': 3148,\n",
              " 'judg': 4613,\n",
              " 'fact': 3066,\n",
              " 'alway': 258,\n",
              " 'nose': 5883,\n",
              " 'took': 8989,\n",
              " 'bite': 814,\n",
              " 'chocol': 1527,\n",
              " 'oreo': 6070,\n",
              " 'taken': 8673,\n",
              " 'aback': 1,\n",
              " 'expect': 3013,\n",
              " 'chunk': 1569,\n",
              " 'peanut': 6329,\n",
              " 'butter': 1149,\n",
              " 'creme': 2031,\n",
              " 'kosher': 4768,\n",
              " 'bagel': 559,\n",
              " 'twice': 9173,\n",
              " 'day': 2197,\n",
              " 'still': 8364,\n",
              " 'quit': 6970,\n",
              " 'chewi': 1477,\n",
              " 'second': 7660,\n",
              " 'delay': 2274,\n",
              " 'plus': 6581,\n",
              " 'werent': 9688,\n",
              " 'new': 5805,\n",
              " 'includ': 4319,\n",
              " 'compens': 1772,\n",
              " 'thank': 8823,\n",
              " 'enclos': 2834,\n",
              " 'card': 1258,\n",
              " 'mark': 5251,\n",
              " 'locat': 5039,\n",
              " 'map': 5226,\n",
              " 'offer': 5996,\n",
              " 'varieti': 9430,\n",
              " 'poppi': 6643,\n",
              " 'urg': 9383,\n",
              " 'seller': 7686,\n",
              " 'perhap': 6395,\n",
              " 'vacuum': 9409,\n",
              " 'sinc': 7890,\n",
              " 'opinion': 6044,\n",
              " 'mayb': 5315,\n",
              " 'intend': 4445,\n",
              " 'free': 3403,\n",
              " 'reduct': 7130,\n",
              " 'previous': 6751,\n",
              " 'wheat': 9704,\n",
              " 'corn': 1931,\n",
              " 'gluten': 3642,\n",
              " 'brewer': 1014,\n",
              " 'garlic': 3517,\n",
              " 'roll': 7394,\n",
              " 'whole': 9747,\n",
              " 'grain': 3725,\n",
              " 'aroma': 401,\n",
              " 'jelli': 4561,\n",
              " 'hard': 3938,\n",
              " 'set': 7727,\n",
              " 'tabl': 8650,\n",
              " 'garden': 3512,\n",
              " 'brunch': 1076,\n",
              " 'wed': 9664,\n",
              " 'town': 9030,\n",
              " 'terribl': 8805,\n",
              " 'enrich': 2863,\n",
              " 'shelter': 7770,\n",
              " 'cut': 2126,\n",
              " 'tie': 8916,\n",
              " 'togeth': 8971,\n",
              " 'spray': 8243,\n",
              " 'crazi': 2014,\n",
              " 'aloha': 233,\n",
              " 'soy': 8150,\n",
              " 'sauc': 7562,\n",
              " 'fabul': 3058,\n",
              " 'overpow': 6148,\n",
              " 'sodium': 8077,\n",
              " 'name': 5723,\n",
              " 'sweet': 8611,\n",
              " 'caramel': 1251,\n",
              " 'excel': 2988,\n",
              " 'substitut': 8486,\n",
              " 'mild': 5459,\n",
              " 'marmalad': 5257,\n",
              " 'read': 7047,\n",
              " 'least': 4889,\n",
              " 'bitter': 817,\n",
              " 'peel': 6347,\n",
              " 'come': 1745,\n",
              " 'list': 5013,\n",
              " 'balanc': 571,\n",
              " 'thick': 8854,\n",
              " 'everyon': 2966,\n",
              " 'specialti': 8180,\n",
              " 'isnt': 4507,\n",
              " 'dalfour': 2153,\n",
              " 'repres': 7230,\n",
              " 'type': 9187,\n",
              " 'antioxid': 331,\n",
              " 'rather': 7031,\n",
              " 'neither': 5775,\n",
              " 'daughter': 2190,\n",
              " 'nor': 5873,\n",
              " 'remaind': 7196,\n",
              " 'school': 7609,\n",
              " 'student': 8449,\n",
              " 'share': 7754,\n",
              " 'among': 280,\n",
              " 'teenag': 8767,\n",
              " 'review': 7300,\n",
              " 'popcorn': 6641,\n",
              " 'top': 8999,\n",
              " 'movi': 5647,\n",
              " 'miss': 5518,\n",
              " 'produt': 6799,\n",
              " 'tablespoon': 8651,\n",
              " 'end': 2838,\n",
              " 'wast': 9632,\n",
              " 'tube': 9136,\n",
              " 'melt': 5378,\n",
              " 'gooey': 3687,\n",
              " 'fruit': 3448,\n",
              " 'couldnt': 1963,\n",
              " 'hate': 3968,\n",
              " 'blame': 830,\n",
              " 'gummi': 3857,\n",
              " 'reorder': 7216,\n",
              " 'mommi': 5570,\n",
              " 'grab': 3716,\n",
              " 'let': 4936,\n",
              " 'arent': 389,\n",
              " 'cheap': 1427,\n",
              " 'babi': 539,\n",
              " 'know': 4742,\n",
              " 'worri': 9843,\n",
              " 'choke': 1536,\n",
              " 'fast': 3109,\n",
              " 'other': 6097,\n",
              " 'pound': 6684,\n",
              " 'pleasant': 6562,\n",
              " 'much': 5658,\n",
              " 'part': 6271,\n",
              " 'adjust': 91,\n",
              " 'bottl': 941,\n",
              " 'easili': 2732,\n",
              " 'pocket': 6586,\n",
              " 'notic': 5894,\n",
              " 'diet': 2398,\n",
              " 'etc': 2937,\n",
              " 'usual': 9398,\n",
              " 'experi': 3018,\n",
              " 'immedi': 4280,\n",
              " 'given': 3603,\n",
              " 'soda': 8074,\n",
              " 'realiz': 7056,\n",
              " 'sugar': 8513,\n",
              " 'depend': 2316,\n",
              " 'person': 6419,\n",
              " 'fruiti': 3450,\n",
              " 'anyon': 340,\n",
              " 'spend': 8195,\n",
              " 'money': 5575,\n",
              " 'save': 7573,\n",
              " 'cube': 2088,\n",
              " 'disappear': 2444,\n",
              " 'shelv': 7772,\n",
              " 'far': 3099,\n",
              " 'larg': 4834,\n",
              " 'citi': 1596,\n",
              " 'yay': 9897,\n",
              " 'roland': 7392,\n",
              " 'similar': 7876,\n",
              " 'europ': 2947,\n",
              " 'white': 9738,\n",
              " 'brown': 1069,\n",
              " 'crush': 2080,\n",
              " 'tomato': 8978,\n",
              " 'textur': 8819,\n",
              " 'cook': 1903,\n",
              " 'wonder': 9821,\n",
              " 'sale': 7506,\n",
              " 'januari': 4545,\n",
              " 'hot': 4166,\n",
              " 'enjoy': 2857,\n",
              " 'right': 7336,\n",
              " 'alreadi': 243,\n",
              " 'stop': 8391,\n",
              " 'core': 1926,\n",
              " 'formula': 3359,\n",
              " 'thorough': 8876,\n",
              " 'pick': 6477,\n",
              " 'leav': 4892,\n",
              " 'wet': 9696,\n",
              " 'protein': 6844,\n",
              " 'stapl': 8301,\n",
              " 'ate': 468,\n",
              " 'later': 4845,\n",
              " 'clear': 1631,\n",
              " 'fed': 3141,\n",
              " 'threw': 8888,\n",
              " 'higher': 4074,\n",
              " 'acclim': 39,\n",
              " 'option': 6053,\n",
              " 'shed': 7763,\n",
              " 'sad': 7480,\n",
              " 'wont': 9824,\n",
              " 'agre': 149,\n",
              " 'chanc': 1399,\n",
              " 'wrong': 9871,\n",
              " 'what': 9701,\n",
              " 'caus': 1325,\n",
              " 'speak': 8173,\n",
              " 'ill': 4267,\n",
              " 'updat': 9368,\n",
              " 'suffer': 8509,\n",
              " 'acid': 58,\n",
              " 'reflux': 7139,\n",
              " 'human': 4201,\n",
              " 'rich': 7321,\n",
              " 'flare': 3244,\n",
              " 'awesom': 529,\n",
              " 'wife': 9768,\n",
              " 'lap': 4828,\n",
              " 'band': 587,\n",
              " 'surgeri': 8579,\n",
              " 'meal': 5330,\n",
              " 'greek': 3778,\n",
              " 'yogurt': 9924,\n",
              " 'hidden': 4069,\n",
              " 'valley': 9414,\n",
              " 'ranch': 7010,\n",
              " 'salad': 7502,\n",
              " 'dress': 2638,\n",
              " 'item': 4519,\n",
              " 'sprout': 8256,\n",
              " 'stuck': 8448,\n",
              " 'addit': 83,\n",
              " 'packet': 6187,\n",
              " 'creami': 2017,\n",
              " 'base': 625,\n",
              " 'inform': 4381,\n",
              " 'cartridg': 1292,\n",
              " 'call': 1193,\n",
              " 'told': 8974,\n",
              " 'care': 1266,\n",
              " 'hodgson': 4104,\n",
              " 'result': 7278,\n",
              " 'loaf': 5033,\n",
              " 'fall': 3082,\n",
              " 'apart': 345,\n",
              " 'honey': 4129,\n",
              " 'bad': 556,\n",
              " 'risk': 7352,\n",
              " 'arriv': 407,\n",
              " 'glad': 3610,\n",
              " 'holiday': 4111,\n",
              " 'finger': 3211,\n",
              " 'solid': 8090,\n",
              " 'bulli': 1110,\n",
              " 'skin': 7924,\n",
              " 'cartilag': 1289,\n",
              " 'wherea': 9713,\n",
              " 'normal': 5877,\n",
              " 'pretti': 6745,\n",
              " 'crunchier': 2078,\n",
              " 'pig': 6490,\n",
              " 'ear': 2717,\n",
              " 'oppos': 6046,\n",
              " 'month': 5595,\n",
              " 'belgian': 705,\n",
              " 'shepherd': 7775,\n",
              " 'huge': 4192,\n",
              " 'averag': 513,\n",
              " 'hell': 4037,\n",
              " 'finish': 3215,\n",
              " 'away': 527,\n",
              " 'minut': 5496,\n",
              " 'chew': 1473,\n",
              " 'braid': 975,\n",
              " 'twist': 9179,\n",
              " 'longer': 5052,\n",
              " 'home': 4118,\n",
              " 'night': 5833,\n",
              " 'entir': 2874,\n",
              " 'inch': 4315,\n",
              " 'less': 4931,\n",
              " 'anywher': 344,\n",
              " 'stunk': 8459,\n",
              " 'high': 4073,\n",
              " 'polici': 6605,\n",
              " 'suck': 8500,\n",
              " 'restock': 7272,\n",
              " 'fee': 3144,\n",
              " 'hope': 4145,\n",
              " 'sick': 7850,\n",
              " 'desper': 2343,\n",
              " 'frustrat': 3452,\n",
              " 'throw': 8897,\n",
              " 'burnt': 1135,\n",
              " 'various': 9431,\n",
              " 'expens': 3016,\n",
              " 'kona': 4755,\n",
              " 'sore': 8129,\n",
              " 'cri': 2037,\n",
              " 'thrown': 8899,\n",
              " 'yet': 9915,\n",
              " 'grew': 3786,\n",
              " 'recent': 7081,\n",
              " 'near': 5750,\n",
              " 'matter': 5302,\n",
              " 'research': 7244,\n",
              " 'today': 8964,\n",
              " 'decid': 2233,\n",
              " 'roast': 7368,\n",
              " 'merchandis': 5394,\n",
              " 'quantiti': 6945,\n",
              " 'process': 6789,\n",
              " 'pride': 6759,\n",
              " 'truli': 9123,\n",
              " 'ridicul': 7335,\n",
              " 'version': 9481,\n",
              " 'load': 5032,\n",
              " 'artifici': 423,\n",
              " 'whove': 9760,\n",
              " 'eaten': 2738,\n",
              " 'bread': 989,\n",
              " 'hand': 3912,\n",
              " 'machin': 5138,\n",
              " 'succumb': 8498,\n",
              " 'mass': 5284,\n",
              " 'inferior': 4370,\n",
              " 'began': 693,\n",
              " 'decad': 2221,\n",
              " 'habit': 3878,\n",
              " 'spit': 8216,\n",
              " 'said': 7498,\n",
              " 'macaroni': 5133,\n",
              " 'healthfood': 3994,\n",
              " 'dish': 2479,\n",
              " 'mac': 5128,\n",
              " 'cold': 1716,\n",
              " 'loos': 5061,\n",
              " 'shape': 7752,\n",
              " 'dislik': 2489,\n",
              " 'pasta': 6288,\n",
              " 'goo': 3682,\n",
              " 'water': 9635,\n",
              " 'rins': 7343,\n",
              " 'ad': 77,\n",
              " 'honest': 4127,\n",
              " 'deal': 2205,\n",
              " 'goug': 3710,\n",
              " 'exceed': 2987,\n",
              " 'bug': 1096,\n",
              " 'afraid': 130,\n",
              " 'flour': 3291,\n",
              " 'clean': 1625,\n",
              " 'though': 8877,\n",
              " 'left': 4905,\n",
              " 'porch': 6649,\n",
              " 'smaller': 7990,\n",
              " 'subtl': 8488,\n",
              " 'gotten': 3708,\n",
              " 'bake': 567,\n",
              " 'ultim': 9202,\n",
              " 'flea': 3263,\n",
              " 'live': 5025,\n",
              " 'california': 1192,\n",
              " 'sand': 7532,\n",
              " 'patio': 6305,\n",
              " 'screen': 7638,\n",
              " 'four': 3374,\n",
              " 'move': 5645,\n",
              " 'continu': 1881,\n",
              " 'trap': 9063,\n",
              " 'child': 1494,\n",
              " 'felt': 3154,\n",
              " 'avoid': 519,\n",
              " 'caught': 1323,\n",
              " 'occasion': 5978,\n",
              " 'random': 7015,\n",
              " 'creepi': 2029,\n",
              " 'attract': 486,\n",
              " 'tuck': 9137,\n",
              " 'sight': 7860,\n",
              " 'corner': 1933,\n",
              " 'pet': 6432,\n",
              " 'curious': 2112,\n",
              " 'refil': 7135,\n",
              " 'weve': 9700,\n",
              " 'bulb': 1102,\n",
              " 'regular': 7167,\n",
              " 'light': 4968,\n",
              " 'led': 4898,\n",
              " 'kashi': 4661,\n",
              " 'fat': 3112,\n",
              " 'ingredi': 4387,\n",
              " 'recogniz': 7092,\n",
              " 'soft': 8079,\n",
              " 'unusu': 9357,\n",
              " 'cheesecak': 1449,\n",
              " 'hazelnut': 3983,\n",
              " 'serv': 7722,\n",
              " 'guess': 3840,\n",
              " 'virtual': 9534,\n",
              " 'ran': 7009,\n",
              " 'compet': 1773,\n",
              " 'red': 7117,\n",
              " 'curri': 2116,\n",
              " 'shrimp': 7835,\n",
              " 'entre': 2879,\n",
              " 'simmer': 7878,\n",
              " 'meat': 5340,\n",
              " 'veget': 9443,\n",
              " 'lite': 5017,\n",
              " 'milk': 5465,\n",
              " 'creat': 2020,\n",
              " 'word': 9833,\n",
              " 'substitu': 8485,\n",
              " 'lbs': 4871,\n",
              " 'bland': 833,\n",
              " 'patak': 6296,\n",
              " 'messag': 5409,\n",
              " 'talk': 8676,\n",
              " 'indian': 4340,\n",
              " 'organ': 6072,\n",
              " 'week': 9670,\n",
              " 'friday': 3422,\n",
              " 'morn': 5609,\n",
              " 'board': 878,\n",
              " 'posit': 6662,\n",
              " 'feedback': 3146,\n",
              " 'starbuck': 8303,\n",
              " 'cafe': 1177,\n",
              " 'verona': 9477,\n",
              " 'famous': 3089,\n",
              " 'show': 7831,\n",
              " 'bulk': 1105,\n",
              " 'help': 4042,\n",
              " 'econom': 2748,\n",
              " 'supermarket': 8560,\n",
              " 'instead': 4430,\n",
              " 'inflat': 4376,\n",
              " 'drawer': 2631,\n",
              " 'sit': 7903,\n",
              " 'keurig': 4697,\n",
              " 'coffer': 1705,\n",
              " 'ecstat': 2750,\n",
              " 'hold': 4107,\n",
              " 'everyth': 2967,\n",
              " 'energi': 2849,\n",
              " 'foami': 3307,\n",
              " 'carbon': 1256,\n",
              " 'vitamin': 9549,\n",
              " 'cheaper': 1429,\n",
              " 'easier': 2730,\n",
              " 'harvest': 3959,\n",
              " 'seed': 7668,\n",
              " 'final': 3205,\n",
              " 'tripl': 9100,\n",
              " 'exclus': 2995,\n",
              " 'begun': 697,\n",
              " 'spoil': 8226,\n",
              " 'pie': 6485,\n",
              " 'fill': 3197,\n",
              " 'oregon': 6069,\n",
              " 'rais': 6999,\n",
              " 'guarante': 3835,\n",
              " 'ginger': 3595,\n",
              " 'ale': 194,\n",
              " 'abl': 11,\n",
              " 'toler': 8975,\n",
              " 'beverag': 755,\n",
              " 'bubbl': 1083,\n",
              " 'bourbon': 955,\n",
              " 'laugh': 4852,\n",
              " 'properti': 6831,\n",
              " 'flu': 3296,\n",
              " 'vernor': 9476,\n",
              " 'pale': 6203,\n",
              " 'comparison': 1767,\n",
              " 'proceed': 6788,\n",
              " 'husband': 4222,\n",
              " 'hes': 4059,\n",
              " 'shown': 7833,\n",
              " 'compost': 1791,\n",
              " 'bin': 782,\n",
              " 'tip': 8947,\n",
              " 'hat': 3966,\n",
              " 'brilliant': 1029,\n",
              " 'gold': 3674,\n",
              " 'spici': 8201,\n",
              " 'amber': 266,\n",
              " 'shake': 7743,\n",
              " 'fist': 3229,\n",
              " 'strict': 8427,\n",
              " 'distribut': 2527,\n",
              " 'admit': 95,\n",
              " 'lone': 5050,\n",
              " 'respons': 7267,\n",
              " 'somewhat': 8106,\n",
              " 'imagin': 4274,\n",
              " 'employe': 2823,\n",
              " 'island': 4506,\n",
              " 'paid': 6191,\n",
              " 'nowher': 5909,\n",
              " 'phenomen': 6452,\n",
              " 'retail': 7281,\n",
              " 'rival': 7359,\n",
              " 'accept': 34,\n",
              " 'due': 2680,\n",
              " 'specif': 8181,\n",
              " 'albeit': 188,\n",
              " 'medium': 5358,\n",
              " 'favor': 3126,\n",
              " 'hesit': 4060,\n",
              " 'refund': 7148,\n",
              " 'true': 9116,\n",
              " 'claim': 1605,\n",
              " 'satisfact': 7557,\n",
              " 'tapioca': 8706,\n",
              " 'extra': 3045,\n",
              " 'generic': 3550,\n",
              " 'spice': 8199,\n",
              " 'notat': 5889,\n",
              " 'individu': 4349,\n",
              " 'fri': 3420,\n",
              " 'potato': 6673,\n",
              " 'hubbi': 4188,\n",
              " 'fool': 3328,\n",
              " 'bar': 596,\n",
              " 'contain': 1870,\n",
              " 'partial': 6275,\n",
              " 'hydrogen': 4230,\n",
              " 'oil': 6006,\n",
              " 'seven': 7730,\n",
              " 'gram': 3728,\n",
              " 'becom': 676,\n",
              " 'english': 2855,\n",
              " 'breakfast': 998,\n",
              " 'howev': 4183,\n",
              " 'rural': 7453,\n",
              " 'happi': 3931,\n",
              " 'job': 4587,\n",
              " 'struck': 8441,\n",
              " 'instruct': 4433,\n",
              " 'indic': 4342,\n",
              " 'powder': 6689,\n",
              " 'patti': 6309,\n",
              " 'conveni': 1892,\n",
              " 'expir': 3022,\n",
              " 'date': 2189,\n",
              " 'complain': 1776,\n",
              " 'happen': 3929,\n",
              " 'chestnut': 1472,\n",
              " 'costco': 1954,\n",
              " 'rememb': 7200,\n",
              " 'stock': 8375,\n",
              " 'onlin': 6032,\n",
              " 'figur': 3193,\n",
              " 'term': 8801,\n",
              " 'relationship': 7180,\n",
              " 'prompt': 6821,\n",
              " 'tore': 9003,\n",
              " 'eager': 2715,\n",
              " 'saw': 7578,\n",
              " 'mushi': 5693,\n",
              " 'reluct': 7194,\n",
              " 'rotten': 7421,\n",
              " 'affect': 122,\n",
              " 'spirit': 8214,\n",
              " 'sold': 8087,\n",
              " 'knock': 4739,\n",
              " 'serious': 7718,\n",
              " 'spread': 8245,\n",
              " 'banana': 586,\n",
              " 'pretzel': 6748,\n",
              " 'cracker': 1995,\n",
              " 'sweeter': 8613,\n",
              " 'consist': 1856,\n",
              " 'per': 6380,\n",
              " 'check': 1436,\n",
              " 'ting': 8939,\n",
              " 'kitten': 4728,\n",
              " 'troubl': 9111,\n",
              " 'diarrhea': 2389,\n",
              " 'medic': 5352,\n",
              " 'allergi': 222,\n",
              " 'sensit': 7701,\n",
              " 'gradual': 3721,\n",
              " 'switch': 8631,\n",
              " 'success': 8496,\n",
              " 'slow': 7979,\n",
              " 'eventu': 2959,\n",
              " 'plan': 6542,\n",
              " 'nutrit': 5945,\n",
              " 'content': 1875,\n",
              " 'premium': 6728,\n",
              " 'vet': 9486,\n",
              " 'coat': 1681,\n",
              " 'shini': 7792,\n",
              " 'weight': 9679,\n",
              " 'picki': 6478,\n",
              " 'limit': 4986,\n",
              " 'thrill': 8891,\n",
              " 'kitti': 4729,\n",
              " 'charg': 1414,\n",
              " 'deliv': 2286,\n",
              " 'door': 2581,\n",
              " 'ask': 440,\n",
              " 'unpop': 9315,\n",
              " 'eden': 2753,\n",
              " 'gourmet': 3712,\n",
              " 'luv': 5122,\n",
              " 'bistro': 811,\n",
              " 'measur': 5339,\n",
              " 'state': 8317,\n",
              " 'usa': 9387,\n",
              " 'appar': 351,\n",
              " ...}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUzpjHAjEiOE"
      },
      "source": [
        "# Defining model for future\n",
        "\n",
        "def train_and_eval(model, trainX, trainY, testX, testY):\n",
        "\n",
        "    # training\n",
        "    _ = model.fit(trainX, trainY)\n",
        "\n",
        "    # predictions\n",
        "    y_preds_train = model.predict(trainX)\n",
        "    y_preds_test = model.predict(testX)\n",
        "\n",
        "    # evaluation\n",
        "    print()\n",
        "    print(model)\n",
        "    print(f\"Train accuracy score : {accuracy_score(trainY, y_preds_train)}\")\n",
        "    print(f\"Test accuracy score : {accuracy_score(testY, y_preds_test)}\")\n",
        "    print(classification_report(testY, y_preds_test))\n",
        "    print('\\n',40*'-')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_WtL7GOEkHW"
      },
      "source": [
        "# Logistic Regression with BoW"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1-OAEEPEqAl",
        "outputId": "152d3e0f-868a-464a-f5fc-0806d0324f72"
      },
      "source": [
        "# Hyperparameters\n",
        "C = [0.001, 0.01, 0.1, 1, 10]\n",
        "\n",
        "for c in C: \n",
        "    # Define model\n",
        "    log_model = LogisticRegression(C=c, max_iter=500, random_state=1)\n",
        "    \n",
        "    # Train and evaluate model\n",
        "    train_and_eval(model=log_model,\n",
        "                   trainX=bow_X_train,\n",
        "                   trainY=y_train,\n",
        "                   testX=bow_X_test,\n",
        "                   testY=y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n",
            "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=1, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "Train accuracy score : 0.80684375\n",
            "Test accuracy score : 0.802375\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.81      0.80      0.80      8000\n",
            "    Positive       0.80      0.81      0.80      8000\n",
            "\n",
            "    accuracy                           0.80     16000\n",
            "   macro avg       0.80      0.80      0.80     16000\n",
            "weighted avg       0.80      0.80      0.80     16000\n",
            "\n",
            "\n",
            " ----------------------------------------\n",
            "\n",
            "LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n",
            "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=1, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "Train accuracy score : 0.840453125\n",
            "Test accuracy score : 0.83025\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.83      0.83      0.83      8000\n",
            "    Positive       0.83      0.83      0.83      8000\n",
            "\n",
            "    accuracy                           0.83     16000\n",
            "   macro avg       0.83      0.83      0.83     16000\n",
            "weighted avg       0.83      0.83      0.83     16000\n",
            "\n",
            "\n",
            " ----------------------------------------\n",
            "\n",
            "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n",
            "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=1, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "Train accuracy score : 0.864\n",
            "Test accuracy score : 0.8348125\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.84      0.83      0.83      8000\n",
            "    Positive       0.83      0.84      0.84      8000\n",
            "\n",
            "    accuracy                           0.83     16000\n",
            "   macro avg       0.83      0.83      0.83     16000\n",
            "weighted avg       0.83      0.83      0.83     16000\n",
            "\n",
            "\n",
            " ----------------------------------------\n",
            "\n",
            "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n",
            "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=1, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "Train accuracy score : 0.8838125\n",
            "Test accuracy score : 0.82725\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.83      0.82      0.83      8000\n",
            "    Positive       0.82      0.84      0.83      8000\n",
            "\n",
            "    accuracy                           0.83     16000\n",
            "   macro avg       0.83      0.83      0.83     16000\n",
            "weighted avg       0.83      0.83      0.83     16000\n",
            "\n",
            "\n",
            " ----------------------------------------\n",
            "\n",
            "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n",
            "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=1, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "Train accuracy score : 0.89584375\n",
            "Test accuracy score : 0.8141875\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.82      0.80      0.81      8000\n",
            "    Positive       0.81      0.82      0.82      8000\n",
            "\n",
            "    accuracy                           0.81     16000\n",
            "   macro avg       0.81      0.81      0.81     16000\n",
            "weighted avg       0.81      0.81      0.81     16000\n",
            "\n",
            "\n",
            " ----------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHsFlLuj4WxD",
        "outputId": "8b44eed2-f3a2-4b33-eaea-d0e95657bad3"
      },
      "source": [
        "# Hyperparameters\n",
        "C = [0.001, 0.01, 0.1, 1, 10]\n",
        "\n",
        "for c in C:\n",
        "  log_model = LogisticRegression(C=c, max_iter=500)\n",
        "  # Train and evaluate model\n",
        "  train_and_eval(model=log_model,\n",
        "                   trainX=bow_X1_train,\n",
        "                   trainY=y1_train,\n",
        "                   testX=bow_X1_test,\n",
        "                   testY=y1_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n",
            "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "Train accuracy score : 0.8726011254461542\n",
            "Test accuracy score : 0.8702591827863508\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.76      0.22      0.34     11274\n",
            "    Positive       0.88      0.99      0.93     62342\n",
            "\n",
            "    accuracy                           0.87     73616\n",
            "   macro avg       0.82      0.60      0.64     73616\n",
            "weighted avg       0.86      0.87      0.84     73616\n",
            "\n",
            "\n",
            " ----------------------------------------\n",
            "\n",
            "LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n",
            "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "Train accuracy score : 0.8933169418021402\n",
            "Test accuracy score : 0.8906351880026081\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.77      0.41      0.54     11274\n",
            "    Positive       0.90      0.98      0.94     62342\n",
            "\n",
            "    accuracy                           0.89     73616\n",
            "   macro avg       0.83      0.69      0.74     73616\n",
            "weighted avg       0.88      0.89      0.88     73616\n",
            "\n",
            "\n",
            " ----------------------------------------\n",
            "\n",
            "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n",
            "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "Train accuracy score : 0.9032910979722272\n",
            "Test accuracy score : 0.89579710932406\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.75      0.48      0.59     11274\n",
            "    Positive       0.91      0.97      0.94     62342\n",
            "\n",
            "    accuracy                           0.90     73616\n",
            "   macro avg       0.83      0.73      0.76     73616\n",
            "weighted avg       0.89      0.90      0.89     73616\n",
            "\n",
            "\n",
            " ----------------------------------------\n",
            "\n",
            "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n",
            "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "Train accuracy score : 0.9085142005223102\n",
            "Test accuracy score : 0.8956476852858074\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.73      0.51      0.60     11274\n",
            "    Positive       0.92      0.97      0.94     62342\n",
            "\n",
            "    accuracy                           0.90     73616\n",
            "   macro avg       0.82      0.74      0.77     73616\n",
            "weighted avg       0.89      0.90      0.89     73616\n",
            "\n",
            "\n",
            " ----------------------------------------\n",
            "\n",
            "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n",
            "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "Train accuracy score : 0.9100661887312751\n",
            "Test accuracy score : 0.8939768528580744\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.71      0.51      0.60     11274\n",
            "    Positive       0.92      0.96      0.94     62342\n",
            "\n",
            "    accuracy                           0.89     73616\n",
            "   macro avg       0.81      0.74      0.77     73616\n",
            "weighted avg       0.89      0.89      0.89     73616\n",
            "\n",
            "\n",
            " ----------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24NTXkbCG4eV"
      },
      "source": [
        "# Naive Bayes Classifier with BoW "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZszWHCQIIcME",
        "outputId": "94e64684-fd15-42fc-9f62-eb1805bc02a8"
      },
      "source": [
        "alphas = [0, 0.2, 0.6, 0.8, 1]\n",
        "\n",
        "for a  in alphas: \n",
        "    # Define model\n",
        "    nb_model = MultinomialNB(alpha=a)\n",
        "\n",
        "    # Train and evaluate model\n",
        "    train_and_eval(model=nb_model,\n",
        "                   trainX=bow_X1_train,\n",
        "                   trainY=y1_train,\n",
        "                   testX=bow_X1_test,\n",
        "                   testY=y1_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "MultinomialNB(alpha=0, class_prior=None, fit_prior=True)\n",
            "Train accuracy score : 0.8839744482291373\n",
            "Test accuracy score : 0.8779205607476636\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.60      0.63      0.61     11274\n",
            "    Positive       0.93      0.92      0.93     62342\n",
            "\n",
            "    accuracy                           0.88     73616\n",
            "   macro avg       0.76      0.78      0.77     73616\n",
            "weighted avg       0.88      0.88      0.88     73616\n",
            "\n",
            "\n",
            " ----------------------------------------\n",
            "\n",
            "MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True)\n",
            "Train accuracy score : 0.8833088252773712\n",
            "Test accuracy score : 0.8778933927407085\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.60      0.63      0.61     11274\n",
            "    Positive       0.93      0.92      0.93     62342\n",
            "\n",
            "    accuracy                           0.88     73616\n",
            "   macro avg       0.76      0.78      0.77     73616\n",
            "weighted avg       0.88      0.88      0.88     73616\n",
            "\n",
            "\n",
            " ----------------------------------------\n",
            "\n",
            "MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True)\n",
            "Train accuracy score : 0.8830541226172566\n",
            "Test accuracy score : 0.8775402086502934\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.59      0.63      0.61     11274\n",
            "    Positive       0.93      0.92      0.93     62342\n",
            "\n",
            "    accuracy                           0.88     73616\n",
            "   macro avg       0.76      0.78      0.77     73616\n",
            "weighted avg       0.88      0.88      0.88     73616\n",
            "\n",
            "\n",
            " ----------------------------------------\n",
            "\n",
            "MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True)\n",
            "Train accuracy score : 0.8829284693049334\n",
            "Test accuracy score : 0.8775945446642034\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.59      0.63      0.61     11274\n",
            "    Positive       0.93      0.92      0.93     62342\n",
            "\n",
            "    accuracy                           0.88     73616\n",
            "   macro avg       0.76      0.78      0.77     73616\n",
            "weighted avg       0.88      0.88      0.88     73616\n",
            "\n",
            "\n",
            " ----------------------------------------\n",
            "\n",
            "MultinomialNB(alpha=1, class_prior=None, fit_prior=True)\n",
            "Train accuracy score : 0.8828571525601013\n",
            "Test accuracy score : 0.8776081286676809\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.59      0.63      0.61     11274\n",
            "    Positive       0.93      0.92      0.93     62342\n",
            "\n",
            "    accuracy                           0.88     73616\n",
            "   macro avg       0.76      0.78      0.77     73616\n",
            "weighted avg       0.88      0.88      0.88     73616\n",
            "\n",
            "\n",
            " ----------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCW3bzQZIglX"
      },
      "source": [
        "# Logistic Regression with Tf-Idf¶"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mCQSRR9Ipao",
        "outputId": "7da03fff-64eb-4f14-80ea-540e6f6af0e9"
      },
      "source": [
        "# Hyperparameters\n",
        "C = [0.001, 0.01, 0.1, 1, 10]\n",
        "\n",
        "for c in C: \n",
        "    # Define model\n",
        "    log_model = LogisticRegression(C=c, max_iter=500, random_state=1)\n",
        "    \n",
        "    # Train and evaluate model\n",
        "    train_and_eval(model=log_model,\n",
        "                   trainX=tfidf_X_train,\n",
        "                   trainY=y_train,\n",
        "                   testX=tfidf_X_test,\n",
        "                   testY=y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n",
            "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=1, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "Train accuracy score : 0.794578125\n",
            "Test accuracy score : 0.788\n",
            "\n",
            " ----------------------------------------\n",
            "\n",
            "LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n",
            "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=1, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "Train accuracy score : 0.809734375\n",
            "Test accuracy score : 0.8035\n",
            "\n",
            " ----------------------------------------\n",
            "\n",
            "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n",
            "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=1, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "Train accuracy score : 0.839109375\n",
            "Test accuracy score : 0.8301875\n",
            "\n",
            " ----------------------------------------\n",
            "\n",
            "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n",
            "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=1, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "Train accuracy score : 0.863265625\n",
            "Test accuracy score : 0.8436875\n",
            "\n",
            " ----------------------------------------\n",
            "\n",
            "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n",
            "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=1, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "Train accuracy score : 0.88425\n",
            "Test accuracy score : 0.8365625\n",
            "\n",
            " ----------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLH1B39BO6DE",
        "outputId": "d8eb8aff-0ac2-4b39-c2c2-6fd02722af2d"
      },
      "source": [
        "# Hyperparameters\n",
        "C = [0.001, 0.01, 0.1, 1, 10]\n",
        "\n",
        "for c in C: \n",
        "    # Define model\n",
        "    log_model = LogisticRegression(C=c, max_iter=500, random_state=1)\n",
        "    \n",
        "    # Train and evaluate model\n",
        "    train_and_eval(model=log_model,\n",
        "                   trainX=tfidf_X1_train,\n",
        "                   trainY=y1_train,\n",
        "                   testX=tfidf_X1_test,\n",
        "                   testY=y1_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n",
            "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=1, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "Train accuracy score : 0.8490020749776711\n",
            "Test accuracy score : 0.8468539447946098\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.00      0.00      0.00     11274\n",
            "    Positive       0.85      1.00      0.92     62342\n",
            "\n",
            "    accuracy                           0.85     73616\n",
            "   macro avg       0.42      0.50      0.46     73616\n",
            "weighted avg       0.72      0.85      0.78     73616\n",
            "\n",
            "\n",
            " ----------------------------------------\n",
            "\n",
            "LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n",
            "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=1, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "Train accuracy score : 0.8555156710056679\n",
            "Test accuracy score : 0.8529124103455771\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.87      0.05      0.09     11274\n",
            "    Positive       0.85      1.00      0.92     62342\n",
            "\n",
            "    accuracy                           0.85     73616\n",
            "   macro avg       0.86      0.52      0.50     73616\n",
            "weighted avg       0.86      0.85      0.79     73616\n",
            "\n",
            "\n",
            " ----------------------------------------\n",
            "\n",
            "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n",
            "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=1, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "Train accuracy score : 0.8915068548975925\n",
            "Test accuracy score : 0.8898880678113453\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.79      0.38      0.51     11274\n",
            "    Positive       0.90      0.98      0.94     62342\n",
            "\n",
            "    accuracy                           0.89     73616\n",
            "   macro avg       0.85      0.68      0.73     73616\n",
            "weighted avg       0.88      0.89      0.87     73616\n",
            "\n",
            "\n",
            " ----------------------------------------\n",
            "\n",
            "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n",
            "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=1, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "Train accuracy score : 0.9051792936925434\n",
            "Test accuracy score : 0.8996957183221039\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.76      0.51      0.61     11274\n",
            "    Positive       0.92      0.97      0.94     62342\n",
            "\n",
            "    accuracy                           0.90     73616\n",
            "   macro avg       0.84      0.74      0.78     73616\n",
            "weighted avg       0.89      0.90      0.89     73616\n",
            "\n",
            "\n",
            " ----------------------------------------\n",
            "\n",
            "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n",
            "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=1, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "Train accuracy score : 0.9109287817401965\n",
            "Test accuracy score : 0.8996821343186264\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.73      0.54      0.62     11274\n",
            "    Positive       0.92      0.96      0.94     62342\n",
            "\n",
            "    accuracy                           0.90     73616\n",
            "   macro avg       0.83      0.75      0.78     73616\n",
            "weighted avg       0.89      0.90      0.89     73616\n",
            "\n",
            "\n",
            " ----------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-c6OVf0_I0-a"
      },
      "source": [
        "# Naive Bayes classifier with Tf-Idf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMM6L1zYJJux",
        "outputId": "3cc97b8e-1c70-404f-ce5f-c002d27ad34d"
      },
      "source": [
        "alphas = [0, 0.2, 0.6, 0.8, 1]\n",
        "\n",
        "for a  in alphas: \n",
        "    # Define model\n",
        "    nb_model = MultinomialNB(alpha=a)\n",
        "\n",
        "    # Train and evaluate model\n",
        "    train_and_eval(model= nb_model,\n",
        "                   trainX=tfidf_X_train,\n",
        "                   trainY=y_train,\n",
        "                   testX=tfidf_X_test,\n",
        "                   testY=y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "MultinomialNB(alpha=0, class_prior=None, fit_prior=True)\n",
            "Train accuracy score : 0.84484375\n",
            "Test accuracy score : 0.810625\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.82      0.80      0.81      8000\n",
            "    Positive       0.80      0.82      0.81      8000\n",
            "\n",
            "    accuracy                           0.81     16000\n",
            "   macro avg       0.81      0.81      0.81     16000\n",
            "weighted avg       0.81      0.81      0.81     16000\n",
            "\n",
            "\n",
            " ----------------------------------------\n",
            "\n",
            "MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True)\n",
            "Train accuracy score : 0.843484375\n",
            "Test accuracy score : 0.8159375\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.82      0.80      0.81      8000\n",
            "    Positive       0.81      0.83      0.82      8000\n",
            "\n",
            "    accuracy                           0.82     16000\n",
            "   macro avg       0.82      0.82      0.82     16000\n",
            "weighted avg       0.82      0.82      0.82     16000\n",
            "\n",
            "\n",
            " ----------------------------------------\n",
            "\n",
            "MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True)\n",
            "Train accuracy score : 0.8424375\n",
            "Test accuracy score : 0.8186875\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.83      0.81      0.82      8000\n",
            "    Positive       0.81      0.83      0.82      8000\n",
            "\n",
            "    accuracy                           0.82     16000\n",
            "   macro avg       0.82      0.82      0.82     16000\n",
            "weighted avg       0.82      0.82      0.82     16000\n",
            "\n",
            "\n",
            " ----------------------------------------\n",
            "\n",
            "MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True)\n",
            "Train accuracy score : 0.841953125\n",
            "Test accuracy score : 0.8186875\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.83      0.81      0.82      8000\n",
            "    Positive       0.81      0.83      0.82      8000\n",
            "\n",
            "    accuracy                           0.82     16000\n",
            "   macro avg       0.82      0.82      0.82     16000\n",
            "weighted avg       0.82      0.82      0.82     16000\n",
            "\n",
            "\n",
            " ----------------------------------------\n",
            "\n",
            "MultinomialNB(alpha=1, class_prior=None, fit_prior=True)\n",
            "Train accuracy score : 0.841859375\n",
            "Test accuracy score : 0.82\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.83      0.81      0.82      8000\n",
            "    Positive       0.81      0.83      0.82      8000\n",
            "\n",
            "    accuracy                           0.82     16000\n",
            "   macro avg       0.82      0.82      0.82     16000\n",
            "weighted avg       0.82      0.82      0.82     16000\n",
            "\n",
            "\n",
            " ----------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FvcvQdWPONw",
        "outputId": "c04eaa91-b92e-43c5-b0e6-5bc350bb58da"
      },
      "source": [
        "alphas = [0, 0.2, 0.6, 0.8, 1]\n",
        "\n",
        "for a  in alphas: \n",
        "    # Define model\n",
        "    nb_model = MultinomialNB(alpha=a)\n",
        "\n",
        "    # Train and evaluate model\n",
        "    train_and_eval(model= nb_model,\n",
        "                   trainX=tfidf_X1_train,\n",
        "                   trainY=y1_train,\n",
        "                   testX=tfidf_X1_test,\n",
        "                   testY=y1_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "MultinomialNB(alpha=0, class_prior=None, fit_prior=True)\n",
            "Train accuracy score : 0.8700778710932857\n",
            "Test accuracy score : 0.865205933492719\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.85      0.15      0.25     11274\n",
            "    Positive       0.87      1.00      0.93     62342\n",
            "\n",
            "    accuracy                           0.87     73616\n",
            "   macro avg       0.86      0.57      0.59     73616\n",
            "weighted avg       0.86      0.87      0.82     73616\n",
            "\n",
            "\n",
            " ----------------------------------------\n",
            "\n",
            "MultinomialNB(alpha=0.2, class_prior=None, fit_prior=True)\n",
            "Train accuracy score : 0.8695209212765018\n",
            "Test accuracy score : 0.8651923494892415\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.86      0.14      0.25     11274\n",
            "    Positive       0.87      1.00      0.93     62342\n",
            "\n",
            "    accuracy                           0.87     73616\n",
            "   macro avg       0.86      0.57      0.59     73616\n",
            "weighted avg       0.86      0.87      0.82     73616\n",
            "\n",
            "\n",
            " ----------------------------------------\n",
            "\n",
            "MultinomialNB(alpha=0.6, class_prior=None, fit_prior=True)\n",
            "Train accuracy score : 0.8683866454301249\n",
            "Test accuracy score : 0.8644588133014562\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.85      0.14      0.24     11274\n",
            "    Positive       0.86      1.00      0.93     62342\n",
            "\n",
            "    accuracy                           0.86     73616\n",
            "   macro avg       0.86      0.57      0.58     73616\n",
            "weighted avg       0.86      0.86      0.82     73616\n",
            "\n",
            "\n",
            " ----------------------------------------\n",
            "\n",
            "MultinomialNB(alpha=0.8, class_prior=None, fit_prior=True)\n",
            "Train accuracy score : 0.8678874282163003\n",
            "Test accuracy score : 0.8641056292110411\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.85      0.14      0.23     11274\n",
            "    Positive       0.86      1.00      0.93     62342\n",
            "\n",
            "    accuracy                           0.86     73616\n",
            "   macro avg       0.86      0.57      0.58     73616\n",
            "weighted avg       0.86      0.86      0.82     73616\n",
            "\n",
            "\n",
            " ----------------------------------------\n",
            "\n",
            "MultinomialNB(alpha=1, class_prior=None, fit_prior=True)\n",
            "Train accuracy score : 0.867340666505921\n",
            "Test accuracy score : 0.8636845251032385\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.85      0.13      0.23     11274\n",
            "    Positive       0.86      1.00      0.93     62342\n",
            "\n",
            "    accuracy                           0.86     73616\n",
            "   macro avg       0.86      0.56      0.58     73616\n",
            "weighted avg       0.86      0.86      0.82     73616\n",
            "\n",
            "\n",
            " ----------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LMM4OuMJPrK"
      },
      "source": [
        "# Logistic Regression with tf-idf is the best model with 84.09% accuracy with c = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRuFzXpfJ1s-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b5c89ab-d241-45f6-8e3f-14ee21715370"
      },
      "source": [
        "### Cross Validation logistic regression with Bow\n",
        "a = np.random.normal(loc = 0 , scale = 0.1,size = 50)\n",
        "param_distb =  {'C': [b for b in a if b >0  ]}\n",
        "print(param_distb)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'C': [0.008348991424093604, 0.18773416426337752, 0.04776982041916623, 0.16683498279972953, 0.024367940612964867, 0.003235191084982228, 0.04483750504964624, 0.13116866448228767, 0.10474107580009939, 0.15154551077290634, 0.14458324875187442, 0.012852975637659594, 0.02895227690208771, 0.009447350873815897, 0.07527775407161082, 0.029869614129635493, 0.17668661559352922, 0.09305752491225072, 0.04268444337179101, 0.17043274883390366, 0.03103021317367809, 0.15440791296659598, 0.08005796362600233, 0.054528156807573225]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zkM9-8q96kO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bc844d4-eb68-4ca8-896b-2f7cde2015b8"
      },
      "source": [
        "model_random = RandomizedSearchCV(LogisticRegression(),param_distb,cv = 10 ,scoring = 'accuracy')\n",
        "model_random.fit(bow_X_train,y_train)\n",
        "print(model_random.best_estimator_)\n",
        "pred = model_random.predict(bow_X_test)\n",
        "print('Accuracy',accuracy_score(y_test,pred)*100)\n",
        "print(classification_report(y_test,pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LogisticRegression(C=0.09305752491225072, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "Accuracy 83.49375\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.84      0.83      0.83      8000\n",
            "    Positive       0.83      0.84      0.84      8000\n",
            "\n",
            "    accuracy                           0.83     16000\n",
            "   macro avg       0.83      0.83      0.83     16000\n",
            "weighted avg       0.83      0.83      0.83     16000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNFjHK6BQFVO",
        "outputId": "556f5896-b075-4a97-c872-332580824f72"
      },
      "source": [
        "model_random = RandomizedSearchCV(LogisticRegression(),param_distb,cv = 10 ,scoring = 'accuracy')\n",
        "model_random.fit(bow_X1_train,y1_train)\n",
        "print(model_random.best_estimator_)\n",
        "pred = model_random.predict(bow_X1_test)\n",
        "print('Accuracy',accuracy_score(y1_test,pred)*100)\n",
        "print(classification_report(y1_test, pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LogisticRegression(C=0.18773416426337752, class_weight=None, dual=False,\n",
            "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
            "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "Accuracy 89.59872853727451\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.74      0.49      0.59     11274\n",
            "    Positive       0.91      0.97      0.94     62342\n",
            "\n",
            "    accuracy                           0.90     73616\n",
            "   macro avg       0.83      0.73      0.77     73616\n",
            "weighted avg       0.89      0.90      0.89     73616\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-KLhOtg-ZfN"
      },
      "source": [
        "def plot_cm(y_true, y_pred):\n",
        "    plt.figure(figsize=(6,6))\n",
        "    \n",
        "    cm = confusion_matrix(y_true, y_pred, normalize='true')\n",
        "    \n",
        "    sns.heatmap(\n",
        "        cm, annot=True, cmap='Blues', cbar=False, fmt='.2f',\n",
        "        xticklabels=labels, yticklabels=labels)\n",
        "    \n",
        "    return plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "id": "05PhvMoBCG4C",
        "outputId": "70acf9b2-9e32-4a1b-b484-a38b9c315a22"
      },
      "source": [
        "  plot_cm(y1_test, pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-11b4fca222c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my1_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-79-ebd4ab5c3712>\u001b[0m in \u001b[0;36mplot_cm\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m      6\u001b[0m     sns.heatmap(\n\u001b[1;32m      7\u001b[0m         \u001b[0mcm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Blues'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'.2f'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         xticklabels=labels, yticklabels=labels)\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'labels' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x432 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNjO_ibfCLuK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f50db24f-4ebb-4908-fd70-be299a04418b"
      },
      "source": [
        "param_grid = {'n_estimators': [8, 16, 32, 64, 100, 200],'max_depth': [2, 5, 7, 9]}\n",
        "gd = GridSearchCV(RandomForestClassifier(class_weight = 'balanced'),param_grid,cv = 5,scoring = 'accuracy')\n",
        "gd.fit(bow_X_train,y_train)\n",
        "print(gd.best_estimator_)\n",
        "pred = gd.predict(bow_X_test)\n",
        "acc = accuracy_score(y_test,pred)\n",
        "print('Accuracy is',acc*100)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
            "                       criterion='gini', max_depth=9, max_features='auto',\n",
            "                       max_leaf_nodes=None, max_samples=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, n_estimators=200,\n",
            "                       n_jobs=None, oob_score=False, random_state=None,\n",
            "                       verbose=0, warm_start=False)\n",
            "Accuracy is 78.825\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "id": "KLaDLO5ponVT",
        "outputId": "28137e16-e445-4a47-eec9-5ffba7b73957"
      },
      "source": [
        "para_grid = {'n_estimators': [8, 16, 32, 64, 100, 200],'max_depth': [10,20]}\n",
        "ggd = GridSearchCV(GradientBoostingClassifier(),para_grid,cv = 5,scoring = 'accuracy')\n",
        "ggd.fit(bow_X_train,y_train)\n",
        "print(ggd.best_estimator_)\n",
        "pred = ggd.predict(bow_X_test)\n",
        "acc = accuracy_score(y_test,pred)\n",
        "print('Accuracy is',acc*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-234be13f7511>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpara_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'n_estimators'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'max_depth'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mggd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGradientBoostingClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpara_grid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscoring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mggd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow_X_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mggd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mggd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow_X_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m   1535\u001b[0m         n_stages = self._fit_stages(\n\u001b[1;32m   1536\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1537\u001b[0;31m             sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\n\u001b[0m\u001b[1;32m   1538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m         \u001b[0;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1592\u001b[0m             raw_predictions = self._fit_stage(\n\u001b[1;32m   1593\u001b[0m                 \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1594\u001b[0;31m                 random_state, X_idx_sorted, X_csc, X_csr)\n\u001b[0m\u001b[1;32m   1595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1596\u001b[0m             \u001b[0;31m# track deviance (= loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[1;32m   1243\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m             tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[0;32m-> 1245\u001b[0;31m                      check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1247\u001b[0m             \u001b[0;31m# update tree leaves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1223\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1225\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1226\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    365\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E51sbLYbEAfq"
      },
      "source": [
        "plot_cm(y_test, pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9C5r8A_JEBms",
        "outputId": "10d72107-8111-4cdf-867e-70ca7b635973"
      },
      "source": [
        "param_grid = {'n_estimators': [8, 16, 32, 64, 100, 200],'max_depth': [2, 5, 7, 9]}\n",
        "gd = GridSearchCV(RandomForestClassifier(class_weight = 'balanced'),param_grid,cv = 5,scoring = 'accuracy')\n",
        "gd.fit(tfidf_X_train,y_train)\n",
        "print(gd.best_estimator_)\n",
        "pred = gd.predict(tfidf_X_test)\n",
        "acc = accuracy_score(y_test,pred)\n",
        "print('Accuracy is',acc*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
            "                       criterion='gini', max_depth=9, max_features='auto',\n",
            "                       max_leaf_nodes=None, max_samples=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, n_estimators=200,\n",
            "                       n_jobs=None, oob_score=False, random_state=None,\n",
            "                       verbose=0, warm_start=False)\n",
            "Accuracy is 79.3625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "p7bKHnb1Mfw6",
        "outputId": "796cc477-f05b-4d2a-8516-b7c72e4d5ad9"
      },
      "source": [
        "plot_cm(y_test, pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nh4_5nFgMjhF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}